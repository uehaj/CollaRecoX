"use client";

import React, { useRef, useState, useCallback, useEffect } from "react";
// yjs and HocuspocusProvider are dynamically imported to avoid SSR localStorage issues
// eslint-disable-next-line @typescript-eslint/no-explicit-any
type YDocType = any;
// eslint-disable-next-line @typescript-eslint/no-explicit-any
type HocuspocusProviderType = any;
import { getAllRecordings, type AudioRecording } from '@/lib/indexedDB';

interface PromptPreset {
  id: string;
  name: string;
  description: string;
  prompt: string;
}

const PROMPT_PRESETS: PromptPreset[] = [
  {
    id: 'none',
    name: 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—',
    description: 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ–‡å­—èµ·ã“ã—',
    prompt: ''
  },
  {
    id: 'filler-removal',
    name: 'ãƒ•ã‚£ãƒ©ãƒ¼é™¤å»',
    description: 'ã€Œãˆãƒ¼ã€ã€Œã‚ã®ãƒ¼ã€ãªã©ã®ãƒ•ã‚£ãƒ©ãƒ¼ã‚’é™¤å»',
    prompt: 'ãƒ•ã‚£ãƒ©ãƒ¼ï¼ˆã€Œãˆãƒ¼ã€ã€Œã‚ã®ãƒ¼ã€ã€Œãã®ã€ã€Œã¾ã‚ã€ãªã©ï¼‰ã‚’é™¤å»ã—ã€æ˜ç­ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚'
  },
  {
    id: 'clean-formal',
    name: 'æ•´ç†ãƒ»æ•¬èªåŒ–',
    description: 'ãƒ•ã‚£ãƒ©ãƒ¼é™¤å» + æ•¬èªã¸ã®å¤‰æ›',
    prompt: 'ãƒ•ã‚£ãƒ©ãƒ¼ã‚’é™¤å»ã—ã€æ•¬èªã‚’ä½¿ã£ãŸä¸å¯§ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã«æ•´ç†ã—ã¦å¤‰æ›ã—ã¦ãã ã•ã„ã€‚'
  },
  {
    id: 'desu-masu',
    name: 'ã§ã™ãƒ»ã¾ã™èª¿ã«å¤‰æ›',
    description: 'æ–‡æœ«ã‚’ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã«çµ±ä¸€',
    prompt: 'æ–‡ç« ã‚’ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ï¼ˆä¸å¯§èªï¼‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚£ãƒ©ãƒ¼ã‚‚é™¤å»ã—ã¦ãã ã•ã„ã€‚'
  },
  {
    id: 'da-dearu',
    name: 'ã ãƒ»ã§ã‚ã‚‹èª¿ã«å¤‰æ›',
    description: 'æ–‡æœ«ã‚’ã€Œã ãƒ»ã§ã‚ã‚‹ã€èª¿ã«çµ±ä¸€',
    prompt: 'æ–‡ç« ã‚’ã€Œã ãƒ»ã§ã‚ã‚‹ã€èª¿ï¼ˆå¸¸ä½“ï¼‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚£ãƒ©ãƒ¼ã‚‚é™¤å»ã—ã¦ãã ã•ã„ã€‚'
  }
];

interface TranscriptionMessage {
  type: 'transcription';
  text: string;
  item_id: string;
}

interface ErrorMessage {
  type: 'error' | 'transcription_error';
  error: string;
  item_id?: string;
}

interface StatusMessage {
  type: 'ready' | 'speech_started' | 'speech_stopped';
  message?: string;
  audio_start_ms?: number;
  audio_end_ms?: number;
  marker?: string | null;
  silence_gap_ms?: number;
  silence_threshold_ms?: number;
}

interface DummyAudioMessage {
  type: 'dummy_audio_started' | 'dummy_audio_completed';
  filename?: string;
  totalSeconds?: number;
}

interface DummyAudioProgressMessage {
  type: 'dummy_audio_progress';
  currentSeconds: number;
  totalSeconds: number;
  progress: number;
}

interface TranscriptionDeltaMessage {
  type: 'transcription_delta';
  delta: string;
  item_id: string;
}

interface ParagraphBreakMessage {
  type: 'paragraph_break';
  marker: string;
}

type WebSocketMessage = TranscriptionMessage | ErrorMessage | StatusMessage | DummyAudioMessage | DummyAudioProgressMessage | TranscriptionDeltaMessage | ParagraphBreakMessage;

export default function RealtimeClient() {
  const websocketRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | AudioWorkletNode | null>(null);
  const recordingStateRef = useRef<boolean>(false);

  // Hocuspocus client refs for test functionality
  const hocuspocusProviderRef = useRef<HocuspocusProviderType | null>(null);
  const hocuspocusDocRef = useRef<YDocType | null>(null);
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const yjsModuleRef = useRef<any>(null);

  const [text, setText] = useState("");
  const textRef = useRef<string>(""); // Keep latest text value for logging
  const [pendingText, setPendingText] = useState(""); // Recognition in progress text
  const pendingItemIdRef = useRef<string | null>(null); // Track current pending item_id

  // Update textRef when text changes
  useEffect(() => {
    textRef.current = text;
  }, [text]);

  // Initialize session ID on component mount
  useEffect(() => {
    if (!currentSessionId) {
      const newSessionId = `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      setCurrentSessionId(newSessionId);
      console.log('[Session] ğŸ†” Auto-generated session ID:', newSessionId);
    }
  }, []); // Empty dependency array - runs only once on mount

  const [isRecording, setIsRecording] = useState(false);
  const [isConnected, setIsConnected] = useState(false);
  const [isConnecting, setIsConnecting] = useState(false); // æ¥ç¶šä¸­ã®çŠ¶æ…‹
  const [error, setError] = useState<string | null>(null);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [audioDevices, setAudioDevices] = useState<MediaDeviceInfo[]>([]);
  const [selectedDeviceId, setSelectedDeviceId] = useState<string>('');
  const [selectedPromptPreset, setSelectedPromptPreset] = useState<string>('none');
  const [customPrompt, setCustomPrompt] = useState<string>('');
  const [promptMode, setPromptMode] = useState<'preset' | 'custom'>('preset');
  const [transcriptionModel, setTranscriptionModel] = useState<string>('gpt-4o-transcribe');
  const [speechBreakDetection, setSpeechBreakDetection] = useState<boolean>(true);
  const [breakMarker, setBreakMarker] = useState<string>('â†©ï¸'); // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ”¹è¡Œçµµæ–‡å­—
  const [vadEnabled, setVadEnabled] = useState<boolean>(true); // VADæœ‰åŠ¹/ç„¡åŠ¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ:æœ‰åŠ¹ï¼‰
  const [vadThreshold, setVadThreshold] = useState<number>(0.5);
  const [vadSilenceDuration, setVadSilenceDuration] = useState<number>(600); // VADç™ºè©±çµ‚äº†åˆ¤å®šæ™‚é–“: OpenAIãŒspeech_stoppedã‚’ç™ºç«ã™ã‚‹ç„¡éŸ³æ™‚é–“ï¼ˆæ¨å¥¨: 500-700msï¼‰
  const [vadPrefixPadding, setVadPrefixPadding] = useState<number>(300);
  const [paragraphBreakThreshold, setParagraphBreakThreshold] = useState<number>(2500); // ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•åŒºåˆ‡ã‚Šåˆ¤å®šæ™‚é–“: ã“ã®æ™‚é–“ä»¥ä¸Šã®ç„¡éŸ³ã§ãƒãƒ¼ã‚«ãƒ¼æŒ¿å…¥ï¼ˆæ¨å¥¨: 2000-2500msï¼‰
  const [currentSessionId, setCurrentSessionId] = useState<string>('');
  const [sessionIdInput, setSessionIdInput] = useState<string>('');
  const [isDummyAudioSending, setIsDummyAudioSending] = useState<boolean>(false);
  const [dummyAudioProgress, setDummyAudioProgress] = useState<{ currentSeconds: number; totalSeconds: number } | null>(null);
  const [localStorageRecordings, setLocalStorageRecordings] = useState<AudioRecording[]>([]);
  const [selectedRecordingId, setSelectedRecordingId] = useState<string>('');
  const [dummySendInterval, setDummySendInterval] = useState<number>(50); // Dummy audio send interval in ms
  const [audioBufferSize, setAudioBufferSize] = useState<number>(8192); // ScriptProcessor buffer size (256, 512, 1024, 2048, 4096, 8192, 16384)
  const [batchMultiplier, setBatchMultiplier] = useState<number>(8); // ä¸€æ‹¬é€ä¿¡ã™ã‚‹éš›ã®ãƒãƒƒãƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ:8=ç´„1365msé–“éš”ï¼‰
  const accumulatedAudioRef = useRef<Int16Array[]>([]); // è“„ç©ç”¨ãƒãƒƒãƒ•ã‚¡
  const [skipSilentChunks, setSkipSilentChunks] = useState<boolean>(false); // ç„¡éŸ³ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ:ç„¡åŠ¹=é«˜ç²¾åº¦ï¼‰
  const [recordingElapsedTime, setRecordingElapsedTime] = useState<number>(0);
  const recordingStartTimeRef = useRef<number>(0);
  const recordingTimerRef = useRef<NodeJS.Timeout | null>(null);
  const autoDisconnectTimerRef = useRef<NodeJS.Timeout | null>(null); // è‡ªå‹•åˆ‡æ–­ã‚¿ã‚¤ãƒãƒ¼
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  const [autoDisconnectDelay, setAutoDisconnectDelay] = useState<number>(30); // è‡ªå‹•åˆ‡æ–­ã¾ã§ã®ç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30ç§’ï¼‰- Story-2ã§UIè¨­å®šã‚’è¿½åŠ äºˆå®š
  const [existingSessionInput, setExistingSessionInput] = useState<string>('');
  const [isEditingSessionId, setIsEditingSessionId] = useState<boolean>(false);
  const [activeSessions, setActiveSessions] = useState<{sessionId: string, connectionCount: number}[]>([]);
  const [isLoadingSessions, setIsLoadingSessions] = useState<boolean>(false);
  const [settingsUpdateMessage, setSettingsUpdateMessage] = useState<string>(''); // è¨­å®šæ›´æ–°ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  const [showClearConfirmDialog, setShowClearConfirmDialog] = useState<boolean>(false); // ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªã‚¢ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°
  const [rewriteModel, setRewriteModel] = useState<string>('gpt-4.1-mini'); // AIå†ç·¨ãƒ¢ãƒ‡ãƒ«

  // Get current prompt for transcription
  const getCurrentPrompt = useCallback((): string => {
    let basePrompt = '';
    
    if (promptMode === 'custom') {
      basePrompt = customPrompt;
    } else {
      const preset = PROMPT_PRESETS.find(p => p.id === selectedPromptPreset);
      basePrompt = preset?.prompt || '';
    }

    return basePrompt;
  }, [promptMode, customPrompt, selectedPromptPreset]);


  // Audio processing utility functions
  const floatTo16BitPCM = useCallback((float32Array: Float32Array): Int16Array => {
    const int16Array = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]));
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16Array;
  }, []);

  const arrayBufferToBase64 = useCallback((buffer: ArrayBuffer): string => {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }, []);

  // Get available audio input devices
  const getAudioDevices = useCallback(async () => {
    try {
      console.log('[Audio Devices] ğŸ¤ Getting available audio input devices...');
      
      if (!navigator?.mediaDevices) {
        console.warn('[Audio Devices] âŒ MediaDevices API not available');
        return;
      }

      // Request permission first
      console.log('[Audio Devices] ğŸ” Requesting microphone permission...');
      await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('[Audio Devices] âœ… Microphone permission granted');
      
      // Get all devices
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter(device => device.kind === 'audioinput');
      console.log('[Audio Devices] ğŸ“‹ Found audio input devices:', audioInputs.map(d => ({ id: d.deviceId, label: d.label })));
      
      setAudioDevices(audioInputs);
      
      // Set default device if none selected
      if (audioInputs.length > 0 && !selectedDeviceId) {
        console.log('[Audio Devices] ğŸ¯ Setting default device:', audioInputs[0].deviceId);
        setSelectedDeviceId(audioInputs[0].deviceId);
      }
    } catch (error) {
      console.error('[Audio Devices] âŒ Error getting audio devices:', error);
      setError('Failed to access audio devices. Please grant microphone permission.');
    }
  }, [selectedDeviceId]);

  // WebSocket connection management - returns Promise for async flow
  const connectWebSocket = useCallback((): Promise<void> => {
    return new Promise((resolve, reject) => {
      if (websocketRef.current?.readyState === WebSocket.OPEN) {
        console.log('[WebSocket] Already connected, skipping connection attempt');
        resolve();
        return;
      }

      setIsConnecting(true);
      const currentPrompt = getCurrentPrompt();
      // Automatically detect protocol and host
      const protocol = typeof window !== 'undefined' && window.location.protocol === 'https:' ? 'wss:' : 'ws:';
      const host = typeof window !== 'undefined' ? window.location.host : 'localhost:8888';
      const wsUrl = `${protocol}//${host}/api/realtime-ws`;
      console.log('[WebSocket] ğŸ”— Connecting to:', wsUrl);
      console.log('[WebSocket] Using transcription prompt:', currentPrompt || '(none)');
      const ws = new WebSocket(wsUrl);
      websocketRef.current = ws;

      ws.onopen = () => {
        console.log('[WebSocket] âœ… Connected successfully');
        setIsConnected(true);
        setIsConnecting(false);
        setError(null);
      
      // Send session ID to server if available
      if (currentSessionId) {
        ws.send(JSON.stringify({
          type: 'set_session_id',
          sessionId: currentSessionId
        }));
        console.log('[WebSocket] ğŸ“‹ Sent session ID to server:', currentSessionId);
      }
      
      // Send prompt configuration to server
      if (currentPrompt) {
        ws.send(JSON.stringify({
          type: 'set_prompt',
          prompt: currentPrompt
        }));
        console.log('[WebSocket] ğŸ“ Sent transcription prompt to server');
      }
      
      // Send transcription model configuration to server
      ws.send(JSON.stringify({
        type: 'set_transcription_model',
        model: transcriptionModel
      }));
      console.log('[WebSocket] ğŸ¤ Sent transcription model to server:', transcriptionModel);
      
      // Send speech break detection settings to server
      ws.send(JSON.stringify({
        type: 'set_speech_break_detection',
        enabled: speechBreakDetection,
        marker: breakMarker
      }));
      console.log('[WebSocket] ğŸ”¸ Sent speech break detection settings:', { enabled: speechBreakDetection, marker: breakMarker });
      
      // Send VAD parameters and paragraph break threshold to server
      // VADç™ºè©±çµ‚äº†åˆ¤å®šæ™‚é–“: OpenAIãŒspeech_stoppedã‚’ç™ºç«ã™ã‚‹ç„¡éŸ³æ™‚é–“
      // ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•åŒºåˆ‡ã‚Šåˆ¤å®šæ™‚é–“: ã“ã®æ™‚é–“ä»¥ä¸Šã®ç„¡éŸ³ã§ãƒãƒ¼ã‚«ãƒ¼æŒ¿å…¥ï¼ˆVADã¨ã¯ç‹¬ç«‹ï¼‰
      ws.send(JSON.stringify({
        type: 'set_vad_params',
        enabled: vadEnabled,
        threshold: vadThreshold,
        silence_duration_ms: vadSilenceDuration,
        prefix_padding_ms: vadPrefixPadding,
        paragraph_break_threshold_ms: paragraphBreakThreshold
      }));
      console.log('[WebSocket] ğŸ›ï¸ Sent VAD parameters:', { enabled: vadEnabled, threshold: vadThreshold, silence_duration_ms: vadSilenceDuration, prefix_padding_ms: vadPrefixPadding, paragraph_break_threshold_ms: paragraphBreakThreshold });

      // Calculate and send commit threshold based on buffer settings
      // Formula: (audioBufferSize * batchMultiplier / 24000) * 1000 milliseconds
      const commitThresholdMs = Math.floor((audioBufferSize * batchMultiplier / 24000) * 1000);
      ws.send(JSON.stringify({
        type: 'set_commit_threshold',
        threshold_ms: commitThresholdMs,
        buffer_size: audioBufferSize,
        batch_multiplier: batchMultiplier
      }));
      console.log('[WebSocket] â±ï¸ Sent commit threshold:', commitThresholdMs, 'ms (buffer:', audioBufferSize, 'samples Ã— batch:', batchMultiplier, ')');

        // Promiseã‚’è§£æ±ºã—ã¦æ¥ç¶šå®Œäº†ã‚’é€šçŸ¥
        resolve();
      };

      ws.onmessage = (event) => {
      try {
        const message: WebSocketMessage = JSON.parse(event.data);
        console.log('[WebSocket] ğŸ“¨ Received message:', message.type, message);
        
        switch (message.type) {
          case 'ready':
            console.log('[Realtime API] ğŸš€ API ready for audio streaming');
            break;
            
          case 'transcription':
            console.log('[Transcription] ğŸ“ Received text:', message.text);
            setText(prev => prev + message.text + ' ');
            // Clear pending text only if it matches the completed item
            if (pendingItemIdRef.current === message.item_id || !message.item_id) {
              setPendingText('');
              pendingItemIdRef.current = null;
            }
            break;

          case 'transcription_delta':
            // Accumulate partial transcription for "recognition in progress" display
            console.log('[Transcription Delta] ğŸ”„ Partial text:', message.delta);
            // Track the item_id for this pending transcription
            if (message.item_id && pendingItemIdRef.current !== message.item_id) {
              // New item started, reset pending text
              setPendingText(message.delta);
              pendingItemIdRef.current = message.item_id;
            } else {
              // Continue accumulating for same item
              setPendingText(prev => prev + message.delta);
            }
            break;

          case 'dummy_audio_started':
            console.log('[Dummy Audio] ğŸµ Started sending dummy audio:', message.filename, 'total:', message.totalSeconds, 'seconds');
            setIsDummyAudioSending(true);
            setDummyAudioProgress({ currentSeconds: 0, totalSeconds: message.totalSeconds || 0 });
            break;

          case 'dummy_audio_progress':
            console.log('[Dummy Audio] ğŸ“Š Progress:', message.currentSeconds.toFixed(2), '/', message.totalSeconds.toFixed(2), 'seconds');
            setDummyAudioProgress({ currentSeconds: message.currentSeconds, totalSeconds: message.totalSeconds });
            break;

          case 'dummy_audio_completed':
            console.log('[Dummy Audio] âœ… Dummy audio processing completed');

            // Log final transcription text
            console.log('[Dummy Audio] ğŸ“ Final transcription text:');
            console.log('====== START OF TRANSCRIPTION ======');
            console.log(text);
            console.log('====== END OF TRANSCRIPTION ======');
            console.log(`[Dummy Audio] ğŸ“Š Total characters: ${text.length}, Total words: ${text.split(/\s+/).filter(word => word.length > 0).length}`);

            setIsDummyAudioSending(false);
            setDummyAudioProgress(null);
            // Stop recording state when dummy audio is completed
            setIsRecording(false);
            setPendingText(''); // Clear pending text on completion
            break;
            
          case 'speech_started':
            setIsSpeaking(true);
            console.log('[Speech Detection] ğŸ¤ Speech started');
            break;
            
          case 'speech_stopped':
            setIsSpeaking(false);
            const silenceGapMs = message.silence_gap_ms || 0;
            const silenceThresholdMs = message.silence_threshold_ms || vadSilenceDuration;
            console.log(`[Speech Detection] ğŸ”‡ Speech stopped (silence: ${silenceGapMs}ms, threshold: ${silenceThresholdMs}ms)`);

            // Insert marker if speech break detection is enabled (for local display)
            // Only insert if actual silence gap exceeds threshold
            if (message.marker && silenceGapMs >= silenceThresholdMs) {
              // âã¯æ”¹è¡Œã®ã¿è¿½åŠ ï¼ˆæ”¹è¡Œè¨˜å·è‡ªä½“ãªã®ã§ãƒãƒ¼ã‚«ãƒ¼è¡¨ç¤ºä¸è¦ï¼‰
              // ãã‚Œä»¥å¤–ï¼ˆâ†©ï¸, ğŸ”„, ğŸ“ãªã©ï¼‰ã¯ãƒãƒ¼ã‚«ãƒ¼+æ”¹è¡Œ
              if (message.marker === 'â') {
                setText(prev => prev + '\n');
                console.log(`[Speech Break] Added newline (silence: ${silenceGapMs}ms >= threshold: ${silenceThresholdMs}ms)`);
              } else {
                setText(prev => prev + ' ' + message.marker + '\n');
                console.log(`[Speech Break] Added marker '${message.marker}' (silence: ${silenceGapMs}ms >= threshold: ${silenceThresholdMs}ms)`);
              }
            } else if (message.marker) {
              // Silence gap below threshold - skip marker insertion (waiting for delayed paragraph_break)
              console.log(`[Speech Break] â³ Waiting for delayed paragraph break (${silenceGapMs}ms < ${silenceThresholdMs}ms)`);
            }
            break;

          case 'paragraph_break':
            // Delayed paragraph break from server (after silence threshold reached)
            if (message.marker) {
              // âã¯æ”¹è¡Œã®ã¿è¿½åŠ ï¼ˆæ”¹è¡Œè¨˜å·è‡ªä½“ãªã®ã§ãƒãƒ¼ã‚«ãƒ¼è¡¨ç¤ºä¸è¦ï¼‰
              // ãã‚Œä»¥å¤–ï¼ˆâ†©ï¸, ğŸ”„, ğŸ“ãªã©ï¼‰ã¯ãƒãƒ¼ã‚«ãƒ¼+æ”¹è¡Œ
              if (message.marker === 'â') {
                setText(prev => prev + '\n');
                console.log(`[Paragraph Break] Added delayed newline`);
              } else {
                setText(prev => prev + ' ' + message.marker + '\n');
                console.log(`[Paragraph Break] Added delayed marker '${message.marker}'`);
              }
            }
            break;

          case 'error':
          case 'transcription_error':
            setError(message.error);
            setIsDummyAudioSending(false);
            // Stop recording state when error occurs
            setIsRecording(false);
            setPendingText(''); // Clear pending text on error
            console.error('[WebSocket] âŒ Error:', message.error);
            break;
            
          default:
            console.log('[WebSocket] â“ Unknown message type:', message);
        }
      } catch (err) {
        console.error('[WebSocket] âŒ Error parsing message:', err, 'Raw data:', event.data);
      }
    };

      ws.onerror = (error) => {
        console.error('[WebSocket] âŒ Connection error:', error);
        setError('WebSocket connection failed');
        setIsConnected(false);
        setIsConnecting(false);
        reject(new Error('WebSocket connection failed'));
      };

      ws.onclose = (event) => {
        console.log('[WebSocket] ğŸ”Œ Connection closed:', event.code, event.reason);
        setIsConnected(false);
        setIsConnecting(false);
        setIsRecording(false);
        // æ¥ç¶šç¢ºç«‹å‰ã«ã‚¯ãƒ­ãƒ¼ã‚ºã•ã‚ŒãŸå ´åˆã¯reject
        if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
          reject(new Error(`WebSocket closed: ${event.code} ${event.reason}`));
        }
      };
    }); // Promiseçµ‚äº†
  }, [getCurrentPrompt, currentSessionId, transcriptionModel, speechBreakDetection, breakMarker, vadEnabled, vadThreshold, vadSilenceDuration, vadPrefixPadding, paragraphBreakThreshold, audioBufferSize, batchMultiplier]);

  const disconnectWebSocket = useCallback(() => {
    // è‡ªå‹•åˆ‡æ–­ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    if (autoDisconnectTimerRef.current) {
      clearTimeout(autoDisconnectTimerRef.current);
      autoDisconnectTimerRef.current = null;
      console.log('[Auto-disconnect] â° Cleared auto-disconnect timer');
    }
    if (websocketRef.current) {
      console.log('[WebSocket] ğŸ”Œ Disconnecting WebSocket');
      websocketRef.current.close();
      websocketRef.current = null;
    }
    setIsConnected(false);
  }, []);

  // è‡ªå‹•åˆ‡æ–­ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹
  const startAutoDisconnectTimer = useCallback(() => {
    // æ—¢å­˜ã®ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    if (autoDisconnectTimerRef.current) {
      clearTimeout(autoDisconnectTimerRef.current);
    }

    console.log(`[Auto-disconnect] â° Starting auto-disconnect timer (${autoDisconnectDelay} seconds)`);

    autoDisconnectTimerRef.current = setTimeout(() => {
      console.log('[Auto-disconnect] â° Auto-disconnect timer expired, disconnecting...');
      disconnectWebSocket();
    }, autoDisconnectDelay * 1000);
  }, [autoDisconnectDelay, disconnectWebSocket]);

  // è‡ªå‹•åˆ‡æ–­ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆéŒ²éŸ³å†é–‹æ™‚ãªã©ï¼‰
  const clearAutoDisconnectTimer = useCallback(() => {
    if (autoDisconnectTimerRef.current) {
      clearTimeout(autoDisconnectTimerRef.current);
      autoDisconnectTimerRef.current = null;
      console.log('[Auto-disconnect] â° Cleared auto-disconnect timer');
    }
  }, []);

  // VADãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•åŒºåˆ‡ã‚Šè¨­å®šã‚’ã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã™ã‚‹é–¢æ•°ï¼ˆæ¥ç¶šä¸­ã«è¨­å®šå¤‰æ›´ã‚’åæ˜ ï¼‰
  const sendVadParamsToServer = useCallback((params: {
    enabled?: boolean;
    threshold?: number;
    silence_duration_ms?: number; // VADç™ºè©±çµ‚äº†åˆ¤å®šæ™‚é–“
    prefix_padding_ms?: number;
    paragraph_break_threshold_ms?: number; // ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•åŒºåˆ‡ã‚Šåˆ¤å®šæ™‚é–“
  }) => {
    if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
      console.log('[VAD Settings] âš ï¸ WebSocket not connected, settings will be applied on next connection');
      return false;
    }

    const message = {
      type: 'set_vad_params',
      ...params
    };

    websocketRef.current.send(JSON.stringify(message));
    console.log('[VAD Settings] ğŸ›ï¸ Sent VAD params to server:', params);

    // ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    setSettingsUpdateMessage('è¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸ');
    setTimeout(() => setSettingsUpdateMessage(''), 3000);

    return true;
  }, []);

  // AIå†ç·¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡ã™ã‚‹é–¢æ•°
  const sendRewriteModelToServer = useCallback((model: string) => {
    if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
      console.log('[Rewrite Settings] âš ï¸ WebSocket not connected, settings will be applied on next connection');
      return false;
    }

    const message = {
      type: 'set_auto_rewrite',
      model: model
    };

    websocketRef.current.send(JSON.stringify(message));
    console.log('[Rewrite Settings] ğŸ¤– Sent rewrite model to server:', model);

    setSettingsUpdateMessage('AIå†ç·¨ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ã—ã¾ã—ãŸ');
    setTimeout(() => setSettingsUpdateMessage(''), 3000);

    return true;
  }, []);

  // ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒå¤‰æ›´ã•ã‚ŒãŸã¨ãã«ã€æ—¢å­˜ã®WebSocketæ¥ç¶šçµŒç”±ã§ã‚µãƒ¼ãƒãƒ¼ã«é€šçŸ¥
  useEffect(() => {
    if (currentSessionId && websocketRef.current?.readyState === WebSocket.OPEN) {
      websocketRef.current.send(JSON.stringify({
        type: 'set_session_id',
        sessionId: currentSessionId
      }));
      console.log('[WebSocket] ğŸ“‹ Sent updated session ID to server:', currentSessionId);
    }
  }, [currentSessionId]);

  // Audio streaming functions
  const startAudioStream = useCallback(async () => {
    try {
      console.log('[Audio] ğŸµ Starting audio stream...');
      
      if (!navigator?.mediaDevices?.getUserMedia) {
        throw new Error('getUserMedia not supported in this browser');
      }

      const audioConstraints: MediaStreamConstraints['audio'] = {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: false,
        // Don't force specific sample rate, let browser choose optimal rate
      };

      // Use selected device if available
      if (selectedDeviceId) {
        console.log('[Audio] ğŸ¤ Using selected device:', selectedDeviceId);
        (audioConstraints as MediaTrackConstraints).deviceId = { exact: selectedDeviceId };
      } else {
        console.warn('[Audio] âš ï¸ No specific device selected, using default');
      }

      console.log('[Audio] ğŸ“‹ Audio constraints:', audioConstraints);
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: audioConstraints
      });
      console.log('[Audio] âœ… Media stream obtained');

      // Enhanced AudioContext compatibility check
      const AudioContextClass = window.AudioContext || (window as unknown as { webkitAudioContext: typeof AudioContext }).webkitAudioContext;
      if (!AudioContextClass) {
        throw new Error('AudioContext not supported in this browser');
      }

      // Create AudioContext for processing (don't force sample rate, let browser decide)
      console.log('[Audio] ğŸ”§ Creating AudioContext...');
      const audioContext = new AudioContextClass();
      audioContextRef.current = audioContext;
      console.log('[Audio] âœ… AudioContext created, sample rate:', audioContext.sampleRate);

      const source = audioContext.createMediaStreamSource(stream);
      console.log('[Audio] ğŸ”Œ Media stream source created');

      // For now, use ScriptProcessor directly to avoid AudioWorklet cache issues
      // TODO: Re-enable AudioWorklet once cache issues are resolved
      console.warn('[Audio] âš ï¸ Using ScriptProcessor for audio processing (AudioWorklet temporarily disabled due to cache issues)');
      
      // Use ScriptProcessor
      if (!audioContext.createScriptProcessor) {
        throw new Error('Audio processing not supported in this browser');
      }
      
      const processor = audioContext.createScriptProcessor(audioBufferSize, 1, 1);
      processorRef.current = processor;
      console.log(`[Audio] ğŸ”§ ScriptProcessor created with ${audioBufferSize} buffer size`);
      
      let audioChunkCount = 0;
      let lastSendTime = 0;          // For timing analysis
      let sendCount = 0;             // Count of actually sent chunks
      let skipCount = 0;             // Count of skipped chunks (silent)
      const timingLog: Array<{timestamp: number; interval: number; type: 'sent' | 'skipped'; samples: number}> = [];

      processor.onaudioprocess = (event) => {
        audioChunkCount++;
        
        // Log more chunks to debug
        if (audioChunkCount <= 20) {
          console.log(`[Audio Processing] ğŸ”„ Event fired! Chunk #${audioChunkCount}`);
          console.log(`[Audio Processing] ğŸ“Š Recording state: ${recordingStateRef.current}, WebSocket ready: ${websocketRef.current?.readyState === WebSocket.OPEN}`);
        }
        
        if (!recordingStateRef.current) {
          console.log(`[Audio Processing] â¸ï¸ Skipping chunk #${audioChunkCount} - recording state is false`);
          return;
        }
        
        if (!websocketRef.current || websocketRef.current.readyState !== WebSocket.OPEN) {
          // WebSocketæœªæ¥ç¶šæ™‚ã¯é™ã‹ã«ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ­ã‚°å‡ºåŠ›ã—ãªã„ï¼‰
          return;
        }

        const inputBuffer = event.inputBuffer;
        const inputData = inputBuffer.getChannelData(0);
        
        // Log every chunk for first 20, then every 20th
        if (audioChunkCount <= 20 || audioChunkCount % 20 === 0) {
          console.log(`[Audio Processing] ğŸµ Processing chunk #${audioChunkCount}, buffer length: ${inputData.length}, sample rate: ${inputBuffer.sampleRate}`);
          
          // Check if there's actual audio data
          const audioLevel = Math.max(...Array.from(inputData).map(Math.abs));
          const avgLevel = Array.from(inputData).reduce((sum, val) => sum + Math.abs(val), 0) / inputData.length;
          console.log(`[Audio Processing] ğŸ“Š Audio level: max=${audioLevel.toFixed(4)}, avg=${avgLevel.toFixed(4)} (0=silence, 1=max)`);
          
          // Log some raw sample values
          if (audioChunkCount <= 5) {
            console.log(`[Audio Processing] ğŸ” First 10 samples:`, Array.from(inputData.slice(0, 10)).map(v => v.toFixed(4)));
          }
          
          // Alert if we're getting silence
          if (audioLevel < 0.001) {
            console.warn(`[Audio Processing] âš ï¸ Very low audio level detected! Check microphone.`);
          }
        }
        
        // Convert to 16-bit PCM at 24kHz (required by OpenAI Realtime API)
        let processedData = inputData;
        
        // Resample from whatever rate to 24kHz
        const targetSampleRate = 24000;
        const sourceSampleRate = inputBuffer.sampleRate;
        
        if (sourceSampleRate !== targetSampleRate) {
          const resampleRatio = targetSampleRate / sourceSampleRate;
          const outputLength = Math.floor(inputData.length * resampleRatio);
          const resampledData = new Float32Array(outputLength);
          
          for (let i = 0; i < outputLength; i++) {
            const sourceIndex = i / resampleRatio;
            const index = Math.floor(sourceIndex);
            const fraction = sourceIndex - index;
            
            if (index + 1 < inputData.length) {
              // Linear interpolation
              resampledData[i] = inputData[index] * (1 - fraction) + inputData[index + 1] * fraction;
            } else {
              resampledData[i] = inputData[index] || 0;
            }
          }
          processedData = resampledData;
        }
        
        const pcm16 = floatTo16BitPCM(processedData);
        const base64Audio = arrayBufferToBase64(pcm16.buffer as ArrayBuffer);

        // Log audio data size and validation
        if (audioChunkCount <= 5 || audioChunkCount % 20 === 0) {
          console.log(`[Audio Processing] ğŸ“¦ PCM16 samples: ${pcm16.length}, Base64 size: ${base64Audio.length} chars`);
          console.log(`[Audio Processing] ğŸ” Sample rate conversion: ${sourceSampleRate}Hz -> ${targetSampleRate}Hz, samples: ${inputData.length} -> ${processedData.length}`);
          
          // Log first few PCM16 values
          if (audioChunkCount <= 3) {
            console.log(`[Audio Processing] ğŸ¯ First 10 PCM16 values:`, Array.from(pcm16.slice(0, 10)));
          }
        }

        // Check for actual audio content before sending
        const maxPcmValue = Math.max(...Array.from(pcm16).map(Math.abs));

        // Skip silent audio chunks if enabled (threshold: 100 for 16-bit PCM)
        if (skipSilentChunks && maxPcmValue < 100) {
          skipCount++;
          const now = performance.now();
          const interval = lastSendTime > 0 ? now - lastSendTime : 0;
          timingLog.push({timestamp: now, interval, type: 'skipped', samples: processedData.length});

          if (audioChunkCount <= 10 || audioChunkCount % 50 === 0) {
            console.warn(`[Audio Processing] âš ï¸ Skipping silent chunk #${audioChunkCount}: max PCM=${maxPcmValue}`);
          }
          return; // Don't send silent audio
        }

        // Batch accumulation logic
        accumulatedAudioRef.current.push(pcm16);

        // Check if we have accumulated enough batches
        if (accumulatedAudioRef.current.length >= batchMultiplier) {
          // Merge accumulated audio chunks into one
          const totalSamples = accumulatedAudioRef.current.reduce((sum, arr) => sum + arr.length, 0);
          const mergedPcm16 = new Int16Array(totalSamples);
          let offset = 0;
          for (const chunk of accumulatedAudioRef.current) {
            mergedPcm16.set(chunk, offset);
            offset += chunk.length;
          }

          // Clear accumulated buffer
          accumulatedAudioRef.current = [];

          // Convert merged audio to base64
          const mergedBase64Audio = arrayBufferToBase64(mergedPcm16.buffer as ArrayBuffer);

          // Send audio chunk to WebSocket
          try {
            const now = performance.now();
            const interval = lastSendTime > 0 ? now - lastSendTime : 0;
            lastSendTime = now;
            sendCount++;

            // Record timing for analysis
            timingLog.push({timestamp: now, interval, type: 'sent', samples: totalSamples});

            websocketRef.current.send(JSON.stringify({
              type: 'audio_chunk',
              audio: mergedBase64Audio
            }));

            // Log with detailed timing info every 10th sent chunk
            if (sendCount % 10 === 0 || sendCount <= 5) {
              console.log(`[Timing Analysis] ğŸ“¤ SENT #${sendCount} | interval: ${interval.toFixed(1)}ms | samples: ${totalSamples} (${batchMultiplier}ãƒãƒƒãƒçµ±åˆ) | max PCM: ${maxPcmValue} | skipped: ${skipCount}`);
            }
          } catch (sendError) {
            console.error(`[WebSocket] âŒ Failed to send audio chunk #${audioChunkCount}:`, sendError);
          }
        } else {
          // Log accumulation progress
          if (audioChunkCount <= 10 || audioChunkCount % 50 === 0) {
            console.log(`[Audio Processing] ğŸ“¦ Accumulating batch ${accumulatedAudioRef.current.length}/${batchMultiplier}`);
          }
        }
      };

      // Create a gain node to prevent audio feedback while still allowing processing
      const gainNode = audioContext.createGain();
      gainNode.gain.value = 0; // Mute the output to prevent feedback
      
      // Critical: ScriptProcessor needs to be connected to destination to fire events
      source.connect(processor);
      processor.connect(gainNode);
      gainNode.connect(audioContext.destination);
      console.log('[Audio] ğŸ”— Audio pipeline connected');
      
      // Additional diagnostics
      console.log('[Audio] ğŸ” AudioContext state:', audioContext.state);
      console.log('[Audio] ğŸ” MediaStream active:', stream.active);
      console.log('[Audio] ğŸ” MediaStream tracks:', stream.getTracks().map(t => ({ kind: t.kind, enabled: t.enabled, readyState: t.readyState })));
      
      // CRITICAL: Resume AudioContext if suspended (required by browsers after user gesture)
      if (audioContext.state === 'suspended') {
        console.log('[Audio] ğŸ”„ Resuming suspended AudioContext...');
        try {
          await audioContext.resume();
          console.log('[Audio] âœ… AudioContext resumed, new state:', audioContext.state);
        } catch (resumeError) {
          console.error('[Audio] âŒ Failed to resume AudioContext:', resumeError);
        }
      }
      
      // Force AudioContext to start processing immediately
      console.log('[Audio] ğŸ¯ Final AudioContext state:', audioContext.state);
      if (audioContext.state !== 'running') {
        console.warn('[Audio] âš ï¸ AudioContext is not running!');
        throw new Error('AudioContext failed to start. State: ' + audioContext.state);
      }

      // Set recording state immediately for audio processing
      recordingStateRef.current = true;

      // Start recording timer
      recordingStartTimeRef.current = Date.now();
      setRecordingElapsedTime(0);
      recordingTimerRef.current = setInterval(() => {
        const elapsed = (Date.now() - recordingStartTimeRef.current) / 1000;
        setRecordingElapsedTime(elapsed);
      }, 100);

      setIsRecording(true);
      console.log('[Audio] âœ… Audio streaming started successfully');
      
      // Test audio processing after a short delay
      setTimeout(() => {
        console.log('[Audio] ğŸ§ª Testing audio processing after 2 seconds...');
        console.log('[Audio] ğŸ” Current AudioContext state:', audioContext.state);
        console.log('[Audio] ğŸ” Current recording state:', isRecording);
        console.log('[Audio] ğŸ” MediaStream still active:', stream.active);
      }, 2000);

    } catch (err) {
      console.error('[Audio] âŒ Error starting audio stream:', err);
      setError(err instanceof Error ? err.message : 'Failed to start audio stream');
    }
  }, [isRecording, floatTo16BitPCM, arrayBufferToBase64, selectedDeviceId, audioBufferSize, batchMultiplier, skipSilentChunks]);

  const stopAudioStream = useCallback(() => {
    console.log('[Audio] ğŸ›‘ Stopping audio stream...');

    // Stop recording state immediately
    recordingStateRef.current = false;

    // Clear accumulated audio buffer
    accumulatedAudioRef.current = [];

    // Stop recording timer
    if (recordingTimerRef.current) {
      clearInterval(recordingTimerRef.current);
      recordingTimerRef.current = null;
    }
    setRecordingElapsedTime(0);

    // Stop audio processing
    if (processorRef.current) {
      console.log('[Audio] ğŸ”Œ Disconnecting audio processor');
      // If it's an AudioWorkletNode, send stop message
      if ('port' in processorRef.current && processorRef.current.port) {
        processorRef.current.port.postMessage({ type: 'stop' });
      }
      processorRef.current.disconnect();
      processorRef.current = null;
    }
    
    if (audioContextRef.current) {
      console.log('[Audio] ğŸ”§ Closing AudioContext');
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    // Don't commit on stop - let the server handle remaining buffer automatically
    // The server will commit when it has enough audio or on timer
    console.log('[Audio] ğŸ“¤ Stopping - server will handle remaining buffer');

    setIsRecording(false);
    setIsSpeaking(false);
    setPendingText(''); // Clear pending text when stopping
    console.log('[Audio] âœ… Audio stream stopped successfully');
  }, []);

  // Function to send status messages to collaborative document
  // Note: Simplified to console.log only to avoid SSR issues with Yjs dynamic import
  const sendStatusToCollaboration = useCallback((message: string) => {
    if (!currentSessionId) {
      return;
    }
    // Log status message - collaborative document updates are handled by server
    console.log('[Hocuspocus Status]', message);
  }, [currentSessionId]);

  // Main control functions
  const startRecording = useCallback(async () => {
    console.log('[Recording] ğŸ™ï¸ Start recording requested');

    // è‡ªå‹•åˆ‡æ–­ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢ï¼ˆéŒ²éŸ³å†é–‹æ™‚ï¼‰
    clearAutoDisconnectTimer();

    // Send detailed start notification to collaborative document
    const currentTime = new Date().toLocaleString('ja-JP');
    const currentPrompt = getCurrentPrompt();
    const promptModeText = promptMode === 'custom' ? 'ã‚«ã‚¹ã‚¿ãƒ ' : 'ãƒ—ãƒªã‚»ãƒƒãƒˆ';
    const promptName = promptMode === 'preset'
      ? PROMPT_PRESETS.find(p => p.id === selectedPromptPreset)?.name || 'ãªã—'
      : 'ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ';

    const statusMessage = `
ğŸ“ æ–‡å­—èµ·ã“ã—é–‹å§‹ (${currentTime})
ğŸ¤ éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«: ${transcriptionModel}
ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š: ${promptModeText} - ${promptName}
${currentPrompt ? `ğŸ“‹ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹: "${currentPrompt}"` : ''}`;
    
    sendStatusToCollaboration(statusMessage);
    
    if (!isConnected && !isConnecting) {
      console.log('[Recording] ğŸ”— Not connected, connecting WebSocket first...');
      try {
        await connectWebSocket();
        console.log('[Recording] âœ… WebSocket connected, starting audio stream');
        startAudioStream();
      } catch (err) {
        console.error('[Recording] âŒ Failed to connect WebSocket:', err);
        setError('WebSocketæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ');
      }
    } else if (isConnecting) {
      console.log('[Recording] â³ Connection already in progress, waiting...');
      // æ¥ç¶šä¸­ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆæ¥ç¶šå®Œäº†å¾Œã«å†åº¦ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã‚‚ã‚‰ã†ï¼‰
    } else {
      console.log('[Recording] ğŸš€ Already connected, starting audio stream immediately');
      startAudioStream();
    }
  }, [isConnected, isConnecting, connectWebSocket, startAudioStream, sendStatusToCollaboration, transcriptionModel, promptMode, selectedPromptPreset, getCurrentPrompt, clearAutoDisconnectTimer]);

  const stopRecording = useCallback(() => {
    console.log('[Recording] â¹ï¸ Stop recording requested');

    // Log final transcription text
    console.log('[Recording] ğŸ“ Final transcription text:');
    console.log('====== START OF TRANSCRIPTION ======');
    console.log(text);
    console.log('====== END OF TRANSCRIPTION ======');
    console.log(`[Recording] ğŸ“Š Total characters: ${text.length}, Total words: ${text.split(/\s+/).filter(word => word.length > 0).length}`);

    // Send detailed stop notification to collaborative document
    const currentTime = new Date().toLocaleString('ja-JP');
    const statusMessage = `
â¹ï¸ æ–‡å­—èµ·ã“ã—çµ‚äº† (${currentTime})
ğŸ¤ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: ${transcriptionModel}`;

    sendStatusToCollaboration(statusMessage);

    stopAudioStream();

    // è‡ªå‹•åˆ‡æ–­ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹ï¼ˆæ¥ç¶šã¯ç¶­æŒã—ã€ä¸€å®šæ™‚é–“å¾Œã«è‡ªå‹•åˆ‡æ–­ï¼‰
    startAutoDisconnectTimer();
  }, [stopAudioStream, sendStatusToCollaboration, transcriptionModel, text, startAutoDisconnectTimer]);

  const clearText = useCallback(() => {
    console.log('[UI] ğŸ§¹ Clearing transcription text');
    setText("");
    setError(null);

    // Clear audio buffer
    if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
      console.log('[Audio] ğŸ—‘ï¸ Clearing server audio buffer');
      websocketRef.current.send(JSON.stringify({
        type: 'clear_audio_buffer'
      }));
    }
  }, []);

  const copyText = useCallback(async () => {
    if (!text) {
      console.log('[UI] âš ï¸ No text to copy');
      return;
    }

    try {
      await navigator.clipboard.writeText(text);
      console.log('[UI] ğŸ“‹ Text copied to clipboard');
      // Show temporary success message
      const originalError = error;
      setError('âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ');
      setTimeout(() => {
        setError(originalError);
      }, 2000);
    } catch (err) {
      console.error('[UI] âŒ Failed to copy text:', err);
      setError('ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ');
    }
  }, [text, error]);

  // Generate or retrieve session ID
  const generateSessionId = useCallback(() => {
    if (!currentSessionId) {
      const newSessionId = `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      setCurrentSessionId(newSessionId);
      return newSessionId;
    }
    return currentSessionId;
  }, [currentSessionId]);

  const createOrOpenEditingSession = useCallback(() => {
    const sessionId = generateSessionId();
    const editorUrl = `${window.location.origin}/editor/${sessionId}`;
    
    console.log('[Session] ğŸš€ Opening editing session:', sessionId);
    console.log('[Session] ğŸ“ Editor URL:', editorUrl);
    
    // Send session ID to server if WebSocket is connected
    if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
      websocketRef.current.send(JSON.stringify({
        type: 'set_session_id',
        sessionId: sessionId
      }));
      console.log('[WebSocket] ğŸ“‹ Updated session ID on server:', sessionId);
    }
    
    // Open new tab with editor
    window.open(editorUrl, '_blank');
  }, [generateSessionId]);

  // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªYjsã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—
  const fetchSessions = useCallback(async () => {
    setIsLoadingSessions(true);
    try {
      const res = await fetch('/api/yjs-sessions');
      const data = await res.json();
      setActiveSessions(data.sessions || []);
    } catch (error) {
      console.error('Failed to fetch sessions:', error);
      setActiveSessions([]);
    } finally {
      setIsLoadingSessions(false);
    }
  }, []);

  // åˆå›ãƒ­ãƒ¼ãƒ‰æ™‚ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—
  useEffect(() => {
    fetchSessions();
  }, [fetchSessions]);

  const connectToExistingSession = useCallback(() => {
    if (existingSessionInput.trim()) {
      let sessionId: string;

      // Handle new session creation
      if (existingSessionInput === '__new__') {
        sessionId = `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        console.log('[Session] ğŸ†• Created new session:', sessionId);
      } else {
        sessionId = existingSessionInput.trim();
        console.log('[Session] ğŸ”— Connected to existing session:', sessionId);
      }

      setCurrentSessionId(sessionId);
      setExistingSessionInput('');

      // Send session ID to server if WebSocket is connected
      if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
        websocketRef.current.send(JSON.stringify({
          type: 'set_session_id',
          sessionId: sessionId
        }));
        console.log('[WebSocket] ğŸ“‹ Updated session ID on server:', sessionId);
      }

    }
  }, [existingSessionInput]);

  const editSessionId = useCallback(() => {
    setSessionIdInput(currentSessionId);
    setIsEditingSessionId(true);
  }, [currentSessionId]);

  const cancelEditSessionId = useCallback(() => {
    setSessionIdInput('');
    setIsEditingSessionId(false);
  }, []);

  const saveSessionId = useCallback(() => {
    if (sessionIdInput.trim()) {
      const sessionId = sessionIdInput.trim();
      setCurrentSessionId(sessionId);
      setIsEditingSessionId(false);
      console.log('[Session] ğŸ’¾ Session ID updated to:', sessionId);
      
      // Send session ID to server if WebSocket is connected
      if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
        websocketRef.current.send(JSON.stringify({
          type: 'set_session_id',
          sessionId: sessionId
        }));
        console.log('[WebSocket] ğŸ“‹ Updated session ID on server:', sessionId);
      }
    }
  }, [sessionIdInput]);

  // Load recordings from IndexedDB
  const loadLocalStorageRecordings = useCallback(async () => {
    try {
      console.log('[IndexedDB] Loading recordings from IndexedDB...');
      const recordings = await getAllRecordings();
      setLocalStorageRecordings(recordings);
      console.log('[IndexedDB] Loaded', recordings.length, 'recordings:', recordings.map((r: AudioRecording) => ({ id: r.id, name: r.name })));
      if (recordings.length > 0 && !selectedRecordingId) {
        setSelectedRecordingId(recordings[0].id);
        console.log('[IndexedDB] Auto-selected first recording:', recordings[0].id);
      }
    } catch (error) {
      console.error('[IndexedDB] Error loading recordings:', error);
      setLocalStorageRecordings([]);
    }
  }, [selectedRecordingId]);

  // Send dummy audio from localStorage
  const sendDummyAudio = useCallback(async () => {
    // Check if recording is selected first
    const recording = localStorageRecordings.find(rec => rec.id === selectedRecordingId);
    if (!recording) {
      setError('é¸æŠã•ã‚ŒãŸéŒ²éŸ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ä½œæˆç”»é¢ã§éŒ²éŸ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚');
      return;
    }

    // è‡ªå‹•åˆ‡æ–­ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
    clearAutoDisconnectTimer();

    // Connect WebSocket if not connected (same as startRecording)
    if (!isConnected && !isConnecting) {
      console.log('[Dummy Audio] ğŸ”— Not connected, connecting WebSocket first...');
      try {
        await connectWebSocket();
        console.log('[Dummy Audio] âœ… WebSocket connected, starting dummy audio send');
      } catch (err) {
        console.error('[Dummy Audio] âŒ Failed to connect WebSocket:', err);
        setError('WebSocketæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ');
        return;
      }
    } else if (isConnecting) {
      console.log('[Dummy Audio] â³ Connection already in progress, please wait...');
      setError('æ¥ç¶šä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚');
      return;
    }

    setIsDummyAudioSending(true);
    setError(null);

    // Start recording state for transcription UI
    setIsRecording(true);

    console.log('[Dummy Audio] ğŸµ Sending localStorage recording:', recording.name, 'interval:', dummySendInterval, 'ms');
    websocketRef.current?.send(JSON.stringify({
      type: 'send_dummy_audio_data',
      audioData: recording.data,
      name: recording.name,
      sendInterval: dummySendInterval
    }));
  }, [isConnected, isConnecting, connectWebSocket, localStorageRecordings, selectedRecordingId, dummySendInterval, clearAutoDisconnectTimer]);

  // Stop dummy audio sending
  const stopDummyAudio = useCallback(() => {
    console.log('[Dummy Audio] ğŸ›‘ Stopping dummy audio sending');

    // Send stop message to server
    if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
      websocketRef.current.send(JSON.stringify({
        type: 'stop_dummy_audio'
      }));
    }

    // Reset states
    setIsDummyAudioSending(false);
    setDummyAudioProgress(null);
    setIsRecording(false);
    setPendingText(''); // Clear pending text when stopping
  }, []);

  // Initialize/cleanup Hocuspocus connection for test functionality
  const initializeHocuspocusClient = useCallback(() => {
    if (!currentSessionId || hocuspocusProviderRef.current) {
      return; // Already initialized or no session
    }

    console.log('[Hocuspocus Test Client] Initializing for session:', currentSessionId);

    // Dynamic import both yjs and HocuspocusProvider to avoid SSR localStorage issues
    Promise.all([
      import('yjs'),
      import('@hocuspocus/provider')
    ]).then(([Y, { HocuspocusProvider }]) => {
      // Store yjs module ref for later use
      yjsModuleRef.current = Y;

      // Create Y.Doc
      const ydoc = new Y.Doc();
      hocuspocusDocRef.current = ydoc;

      // Create provider
      const protocol = typeof window !== 'undefined' && window.location.protocol === 'https:' ? 'wss:' : 'ws:';
      const host = typeof window !== 'undefined' ? window.location.host : 'localhost:8888';
      const websocketUrl = `${protocol}//${host}/api/yjs-ws`;
      const roomName = `transcribe-editor-v2-${currentSessionId}`;

      const provider = new HocuspocusProvider({
        url: websocketUrl,
        name: roomName,
        document: ydoc,
      });

      hocuspocusProviderRef.current = provider;

      provider.on('connect', () => {
        console.log('[Hocuspocus Test Client] Connected to collaborative session');
      });

      provider.on('disconnect', () => {
        console.log('[Hocuspocus Test Client] Disconnected from collaborative session');
      });

      provider.on('error', (error: unknown) => {
        console.error('[Hocuspocus Test Client] Error:', error);
      });
    }).catch((error) => {
      console.error('[Hocuspocus Test Client] Failed to load yjs/HocuspocusProvider:', error);
    });
  }, [currentSessionId]);

  const cleanupHocuspocusClient = useCallback(() => {
    if (hocuspocusProviderRef.current) {
      console.log('[Hocuspocus Test Client] Cleaning up connection');
      hocuspocusProviderRef.current.disconnect();
      hocuspocusProviderRef.current.destroy();
      hocuspocusProviderRef.current = null;
    }
    if (hocuspocusDocRef.current) {
      hocuspocusDocRef.current = null;
    }
  }, []);

  // Test text send function - send directly to Hocuspocus document as a client
  const sendTestText = useCallback(() => {
    if (!currentSessionId) {
      setError('ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«å…±åŒæ ¡æ­£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚');
      return;
    }

    // Initialize Hocuspocus client if not already done
    initializeHocuspocusClient();

    if (!hocuspocusDocRef.current || !hocuspocusProviderRef.current) {
      setError('Hocuspocusã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚');
      return;
    }

    const provider = hocuspocusProviderRef.current;
    
    // Check if provider is connected, if not, wait for connection
    const sendWhenReady = () => {
      const testTexts = [
        'ãƒ†ã‚¹ãƒˆé€ä¿¡1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‹ã‚‰ã®çµ±åˆãƒ†ã‚¹ãƒˆã§ã™ã€‚',
        'ãƒ†ã‚¹ãƒˆé€ä¿¡2: ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å…±åŒæ ¡æ­£ç”»é¢ã«è¡¨ç¤ºã•ã‚Œã‚‹ã¯ãšã§ã™ã€‚',
        'ãƒ†ã‚¹ãƒˆé€ä¿¡3: Hocuspocusã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§åŒæœŸã•ã‚Œã¾ã™ã€‚',
        'ãƒ†ã‚¹ãƒˆé€ä¿¡4: è¤‡æ•°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç¢ºèªã§ãã¾ã™ã€‚'
      ];

      const randomText = testTexts[Math.floor(Math.random() * testTexts.length)];
      
      try {
        // Check if yjs module is loaded
        const Y = yjsModuleRef.current;
        if (!Y) {
          console.error('[Hocuspocus Test Client] Yjs module not loaded yet');
          return;
        }

        // Add text to Hocuspocus document as a collaborative client using TipTap-compatible format
        const fieldName = `content-${currentSessionId}`;

        // TipTap Collaboration uses XmlFragment, not Text
        const fragment = hocuspocusDocRef.current!.getXmlFragment(fieldName);

        // Add text to existing paragraph or create new one if needed
        const hasContent = fragment.length > 0;

        if (hasContent) {
          // Get the last element in the fragment
          const lastElement = fragment.get(fragment.length - 1);

          if (lastElement && lastElement instanceof Y.XmlElement && lastElement.nodeName === 'paragraph') {
            // Add text to the existing last paragraph
            const existingTextNode = lastElement.get(0);
            if (existingTextNode && existingTextNode instanceof Y.XmlText) {
              // Append text with space to existing text node
              existingTextNode.insert(existingTextNode.length, ` ${randomText}`);
            } else {
              // Create new text node in existing paragraph
              const newTextNode = new Y.XmlText();
              newTextNode.insert(0, ` ${randomText}`);
              lastElement.insert(lastElement.length, [newTextNode]);
            }
          } else {
            // Last element is not a paragraph, create new paragraph
            const newParagraph = new Y.XmlElement('paragraph');
            const newTextNode = new Y.XmlText();
            newTextNode.insert(0, ` ${randomText}`);
            newParagraph.insert(0, [newTextNode]);
            fragment.insert(fragment.length, [newParagraph]);
          }
        } else {
          // No content yet, create first paragraph
          const newParagraph = new Y.XmlElement('paragraph');
          const newTextNode = new Y.XmlText();
          newTextNode.insert(0, randomText);
          newParagraph.insert(0, [newTextNode]);
          fragment.insert(0, [newParagraph]);
        }
        
        console.log('[Hocuspocus Test Client] Text sent to collaborative document as paragraph:', randomText);
        console.log('[Hocuspocus Test Client] Fragment length after insert:', fragment.length);
        console.log('[Hocuspocus Test Client] Fragment content preview:', fragment.toString().substring(0, 100) + '...');
        
        // Also add to local display for immediate feedback
        setText(prev => prev + randomText + ' ');
        
        // Clear any previous errors
        setError(null);
      } catch (error) {
        console.error('[Hocuspocus Test Client] Error sending text:', error);
        setError('ãƒ†ã‚­ã‚¹ãƒˆã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚');
      }
    };

    // Always wait for connection to ensure reliable sending
    console.log('[Hocuspocus Test Client] Setting up connection listener for test text sending');
    
    // Try to send immediately first, if that fails, wait for connection
    try {
      sendWhenReady();
      console.log('[Hocuspocus Test Client] Text sent immediately (provider was ready)');
    } catch (error) {
      console.log('[Hocuspocus Test Client] Immediate send failed, waiting for connection...', error);
      
      // Wait for connection and then send
      const onConnect = () => {
        console.log('[Hocuspocus Test Client] Connection established, sending test text');
        provider.off('connect', onConnect);
        try {
          sendWhenReady();
        } catch (retryError) {
          console.error('[Hocuspocus Test Client] Failed to send after connection:', retryError);
          setError('ãƒ†ã‚­ã‚¹ãƒˆã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚');
        }
      };
      
      provider.on('connect', onConnect);
      
      // Timeout after 5 seconds if connection doesn't happen
      setTimeout(() => {
        provider.off('connect', onConnect);
        setError('Hocuspocusæ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã™ã€‚å…±åŒæ ¡æ­£ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒé–‹ã„ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
      }, 5000);
    }
  }, [currentSessionId, initializeHocuspocusClient]);

  // Initialize Hocuspocus when session changes
  useEffect(() => {
    if (currentSessionId) {
      initializeHocuspocusClient();
    } else {
      cleanupHocuspocusClient();
    }
  }, [currentSessionId, initializeHocuspocusClient, cleanupHocuspocusClient]);

  // Load audio devices when component mounts
  useEffect(() => {
    console.log('[Component] ğŸ¬ RealtimeClient component mounted, loading audio devices...');
    getAudioDevices();
    loadLocalStorageRecordings();
  }, [getAudioDevices, loadLocalStorageRecordings]);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      console.log('[Component] ğŸ§¹ RealtimeClient component unmounting, cleaning up...');
      stopAudioStream();
      disconnectWebSocket();
      cleanupHocuspocusClient();
    };
  }, [stopAudioStream, disconnectWebSocket, cleanupHocuspocusClient]);

  return (
    <div className="min-h-screen bg-gray-50">
      {/* Header - editorã¨åŒæ§˜ã®æ§‹é€  */}
      <header className="bg-white shadow-sm border-b">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-4">
          <div className="flex items-center justify-between">
            <div>
              <h1 className="text-2xl font-bold text-gray-900">ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–‡å­—èµ·ã“ã—</h1>
              <p className="text-sm text-gray-600 mt-1">OpenAI Realtime APIã‚’ä½¿ç”¨ã—ãŸéŸ³å£°èªè­˜</p>
            </div>
            <div className="flex items-center space-x-3">
              <a
                href="/dummy-recorder"
                className="px-3 py-1 text-sm bg-green-600 text-white rounded hover:bg-green-700 transition-colors"
              >
                éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ä½œæˆ
              </a>
              <a
                href="/manual.html"
                className="px-3 py-1 text-sm bg-blue-600 text-white rounded hover:bg-blue-700 transition-colors"
                target="_blank"
                rel="noopener noreferrer"
              >
                ãƒãƒ‹ãƒ¥ã‚¢ãƒ«
              </a>
            </div>
          </div>
        </div>
      </header>

      <main className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
        <div className="space-y-6">

        {/* Transcription Model Selection & Prompt Settings - Side by Side */}
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
          {/* Left: éŸ³å£°èªè­˜è¨­å®š */}
          <div className="bg-white p-6 rounded-lg shadow-md">
            <h3 className="text-lg font-medium text-gray-900 mb-4">
              éŸ³å£°èªè­˜è¨­å®š
            </h3>

            {/* Transcription Model Selection */}
            <div className="mb-6">
              <label htmlFor="transcription-model-select" className="block text-sm font-medium text-gray-700 mb-2">
                éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«:
              </label>
              <select
                id="transcription-model-select"
                value={transcriptionModel}
                onChange={(e) => setTranscriptionModel(e.target.value)}
                disabled={isRecording || isConnected}
                className="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
              >
                <option value="whisper-1">
                  Whisper-1 (å¾“æ¥ãƒ¢ãƒ‡ãƒ«)
                </option>
                <option value="gpt-4o-mini-transcribe">
                  GPT-4o Mini Transcribe (è»½é‡ãƒ»é«˜é€Ÿ)
                </option>
                <option value="gpt-4o-transcribe">
                  GPT-4o Transcribe (é«˜ç²¾åº¦)
                </option>
              </select>
            </div>

            {/* Transcription Prompt Settings */}
            <h4 className="text-md font-medium text-gray-800 mb-3 pt-3 border-t border-gray-200">
              æ–‡å­—èµ·ã“ã—ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
            </h4>
          
          {/* Prompt Mode Selection */}
          <div className="mb-4">
            <div className="flex space-x-4">
              <label className="flex items-center">
                <input
                  type="radio"
                  value="preset"
                  checked={promptMode === 'preset'}
                  onChange={(e) => setPromptMode(e.target.value as 'preset' | 'custom')}
                  disabled={isRecording || isConnected}
                  className="mr-2"
                />
                ãƒ—ãƒªã‚»ãƒƒãƒˆä½¿ç”¨
              </label>
              <label className="flex items-center">
                <input
                  type="radio"
                  value="custom"
                  checked={promptMode === 'custom'}
                  onChange={(e) => setPromptMode(e.target.value as 'preset' | 'custom')}
                  disabled={isRecording || isConnected}
                  className="mr-2"
                />
                ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
              </label>
            </div>
          </div>

          {/* Preset Selection */}
          {promptMode === 'preset' && (
            <div className="mb-4">
              <label htmlFor="prompt-preset-select" className="block text-sm font-medium text-gray-700 mb-2">
                ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠ:
              </label>
              <select
                id="prompt-preset-select"
                value={selectedPromptPreset}
                onChange={(e) => setSelectedPromptPreset(e.target.value)}
                disabled={isRecording || isConnected}
                className="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
              >
                {PROMPT_PRESETS.map((preset) => (
                  <option key={preset.id} value={preset.id}>
                    {preset.name} - {preset.description}
                  </option>
                ))}
              </select>
              <div className="mt-2 p-3 bg-gray-50 rounded-md">
                <p className="text-sm text-gray-600">
                  <strong>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹:</strong>
                </p>
                <p className="text-sm text-gray-700 mt-1">
                  {getCurrentPrompt() || 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—'}
                </p>
              </div>
            </div>
          )}

            {/* Custom Prompt Input */}
            {promptMode === 'custom' && (
              <div className="mb-4">
                <label htmlFor="custom-prompt" className="block text-sm font-medium text-gray-700 mb-2">
                  ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:
                </label>
                <textarea
                  id="custom-prompt"
                  value={customPrompt}
                  onChange={(e) => setCustomPrompt(e.target.value)}
                  disabled={isRecording || isConnected}
                  rows={4}
                  className="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                  placeholder="æ–‡å­—èµ·ã“ã—ã‚’ã©ã®ã‚ˆã†ã«å‡¦ç†ã™ã‚‹ã‹ã®æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."
                />
                <p className="text-xs text-gray-500 mt-1">
                  ä¾‹: ã€Œãƒ•ã‚£ãƒ©ãƒ¼ã‚’é™¤å»ã—ã€æ•¬èªã‚’ä½¿ã£ãŸèª­ã¿ã‚„ã™ã„æ–‡ç« ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€
                </p>
              </div>
            )}

            <p className="text-xs text-gray-500 mt-4">
              â€» ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã¯æ¥ç¶šå‰ã«ã®ã¿å¤‰æ›´å¯èƒ½ã§ã™
            </p>
          </div>

          {/* Right: VADè¨­å®š */}
          <div className="bg-white p-6 rounded-lg shadow-md">
            <h3 className="text-lg font-medium text-gray-900 mb-4">
              éŸ³å£°åŒºé–“æ¤œå‡ºï¼ˆVADè¨­å®šï¼‰
            </h3>

            {/* VAD Enable/Disable */}
            <div className="mb-4">
              <label className="flex items-center">
                <input
                  type="checkbox"
                  checked={vadEnabled}
                  onChange={(e) => {
                    const newValue = e.target.checked;
                    setVadEnabled(newValue);
                    // æ¥ç¶šä¸­ãªã‚‰å³åº§ã«ã‚µãƒ¼ãƒãƒ¼ã«é€ä¿¡
                    if (isConnected) {
                      sendVadParamsToServer({ enabled: newValue });
                    }
                  }}
                  disabled={isRecording}
                  className="mr-2 h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300 rounded"
                />
                <span className="text-sm font-medium text-gray-700">
                  VADï¼ˆéŸ³å£°åŒºé–“æ¤œå‡ºï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹
                </span>
              </label>
              <p className="text-xs text-gray-500 mt-1 ml-6">
                VADã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨éŸ³å£°åŒºé–“ãŒè‡ªå‹•æ¤œå‡ºã•ã‚Œã¾ã™ã€‚
              </p>
            </div>

            {/* Speech Break Detection (sub-option of VAD) */}
            {vadEnabled && (
              <div className="mb-4 ml-6 p-3 bg-white rounded-md border border-gray-300">
                <label className="flex items-center">
                  <input
                    type="checkbox"
                    checked={speechBreakDetection}
                    onChange={(e) => setSpeechBreakDetection(e.target.checked)}
                    disabled={isRecording}
                    className="mr-2 h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300 rounded"
                  />
                  <span className="text-sm font-medium text-gray-700">
                    ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•æ¤œå‡ºãƒãƒ¼ã‚«ãƒ¼ã‚’æŒ¿å…¥
                  </span>
                </label>
                <p className="text-xs text-gray-500 mt-1 ml-6">
                  æŒ‡å®šæ™‚é–“ä»¥ä¸Šã®ç„¡éŸ³ã‚’æ¤œå‡ºã—ã¦ãƒãƒ¼ã‚«ãƒ¼æ–‡å­—ã‚’æŒ¿å…¥ã—ã¾ã™
                </p>

                {speechBreakDetection && (
                  <div className="mt-3 ml-6">
                    <label htmlFor="break-marker" className="block text-xs font-medium text-gray-600 mb-1">
                      åŒºåˆ‡ã‚Šãƒãƒ¼ã‚«ãƒ¼:
                    </label>
                    <select
                      id="break-marker"
                      value={breakMarker}
                      onChange={(e) => setBreakMarker(e.target.value)}
                      disabled={isRecording}
                      className="block w-32 px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                    >
                      <option value="â">â (æ”¹è¡Œ)</option>
                      <option value="â†©ï¸">â†©ï¸ (æ”¹è¡Œçµµæ–‡å­—)</option>
                      <option value="ğŸ”„">ğŸ”„ (æ›´æ–°)</option>
                      <option value="ğŸ“">ğŸ“ (ãƒ¡ãƒ¢)</option>
                    </select>
                  </div>
                )}
              </div>
            )}

            <div className={`grid grid-cols-1 md:grid-cols-3 gap-4 ${!vadEnabled ? 'opacity-50' : ''}`}>
              {/* Threshold */}
              <div>
                <label htmlFor="vad-threshold" className="block text-xs font-medium text-gray-600 mb-1 flex items-center">
                  æ¤œå‡ºæ„Ÿåº¦ (Threshold):
                  <span className="ml-1 relative group">
                    <span className="cursor-help text-blue-500 hover:text-blue-700">?</span>
                    <span className="absolute left-1/2 -translate-x-1/2 bottom-full mb-2 w-72 p-2 bg-gray-800 text-white text-xs rounded shadow-lg opacity-0 group-hover:opacity-100 transition-opacity z-50 pointer-events-none whitespace-normal">
                      éŸ³å£°ã¨ã—ã¦æ¤œå‡ºã™ã‚‹æœ€å°ã®éŸ³é‡ãƒ¬ãƒ™ãƒ«ã€‚<br/><br/>
                      <strong>ä½ã„å€¤ (0.1-0.3):</strong> å°ã•ãªéŸ³ã§ã‚‚ã€Œæœ‰éŸ³ã€ã¨æ¤œå‡ºã€‚<br/>
                      â†’ ç„¡éŸ³åŒºé–“ãŒçŸ­ããªã‚‹ï¼ˆç„¡éŸ³æ¤œå‡ºã•ã‚Œã«ãã„ï¼‰<br/><br/>
                      <strong>é«˜ã„å€¤ (0.7-0.9):</strong> æ˜ç¢ºãªç™ºè©±ã®ã¿ã€Œæœ‰éŸ³ã€ã¨æ¤œå‡ºã€‚<br/>
                      â†’ ç„¡éŸ³åŒºé–“ãŒé•·ããªã‚‹ï¼ˆç„¡éŸ³æ¤œå‡ºã•ã‚Œã‚„ã™ã„ï¼‰<br/><br/>
                      <strong>æ¨å¥¨:</strong> é™ã‹ãªç’°å¢ƒ=0.3-0.5ã€ãƒã‚¤ã‚ºç’°å¢ƒ=0.5-0.7
                    </span>
                  </span>
                </label>
                <input
                  id="vad-threshold"
                  type="number"
                  min="0.1"
                  max="1.0"
                  step="0.1"
                  value={vadThreshold}
                  onChange={(e) => {
                    const newValue = parseFloat(e.target.value);
                    setVadThreshold(newValue);
                    if (isConnected) {
                      sendVadParamsToServer({ threshold: newValue });
                    }
                  }}
                  disabled={!vadEnabled || isRecording}
                  className="block w-full px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                />
                <p className="text-xs text-gray-500 mt-1">
                  0.1-1.0 (ä½ã„ã»ã©æ•æ„Ÿ)
                </p>
              </div>

              {/* VAD Silence Duration - OpenAI speech_stopped detection */}
              <div>
                <label htmlFor="vad-silence" className="block text-xs font-medium text-gray-600 mb-1 flex items-center">
                  VADç™ºè©±çµ‚äº†åˆ¤å®š (ms):
                  <span className="ml-1 relative group">
                    <span className="cursor-help text-blue-500 hover:text-blue-700">?</span>
                    <span className="absolute left-1/2 -translate-x-1/2 bottom-full mb-2 w-72 p-2 bg-gray-800 text-white text-xs rounded shadow-lg opacity-0 group-hover:opacity-100 transition-opacity z-50 pointer-events-none whitespace-normal">
                      OpenAI VADãŒç™ºè©±çµ‚äº†(speech_stopped)ã‚’ç™ºç«ã™ã‚‹ç„¡éŸ³æ™‚é–“ã€‚<br/><br/>
                      <strong>çŸ­ã„ (200-500ms):</strong> çŸ­ã„é–“ã§ç™ºè©±åŒºåˆ‡ã‚Šæ¤œçŸ¥<br/>
                      <strong>é•·ã„ (1000-3000ms):</strong> é•·ã„é–“ã‚’è¨±å®¹
                    </span>
                  </span>
                </label>
                <input
                  id="vad-silence"
                  type="number"
                  min="200"
                  max="3000"
                  step="100"
                  value={vadSilenceDuration}
                  onChange={(e) => {
                    const newValue = parseInt(e.target.value);
                    setVadSilenceDuration(newValue);
                    if (isConnected) {
                      sendVadParamsToServer({ silence_duration_ms: newValue });
                    }
                  }}
                  disabled={!vadEnabled || isRecording}
                  className="block w-full px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                />
                <p className="text-xs text-gray-500 mt-1">
                  200-3000ms (VADæœ‰åŠ¹æ™‚ã®ã¿)
                </p>
              </div>

              {/* Paragraph Break Threshold - independent from VAD */}
              <div>
                <label htmlFor="paragraph-break-threshold" className="block text-xs font-medium text-gray-600 mb-1 flex items-center">
                  ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•åŒºåˆ‡ã‚Šåˆ¤å®š (ms):
                  <span className="ml-1 relative group">
                    <span className="cursor-help text-blue-500 hover:text-blue-700">?</span>
                    <span className="absolute left-1/2 -translate-x-1/2 bottom-full mb-2 w-72 p-2 bg-gray-800 text-white text-xs rounded shadow-lg opacity-0 group-hover:opacity-100 transition-opacity z-50 pointer-events-none whitespace-normal">
                      ã“ã®æ™‚é–“ä»¥ä¸Šã®ç„¡éŸ³ãŒã‚ã£ãŸå ´åˆã®ã¿ã€åŒºåˆ‡ã‚Šãƒãƒ¼ã‚«ãƒ¼ã‚’æŒ¿å…¥ã—ã¾ã™ã€‚<br/><br/>
                      VADç™ºè©±çµ‚äº†åˆ¤å®šã¨ã¯ç‹¬ç«‹ã—ãŸè¨­å®šã§ã™ã€‚<br/><br/>
                      <strong>ä¾‹:</strong> VAD=500ms, ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•=3000ms ã®å ´åˆ<br/>
                      â†’ 500msç„¡éŸ³ã§speech_stoppedç™ºç«<br/>
                      â†’ å®Ÿéš›ã®ç„¡éŸ³ãŒ3000msæœªæº€ãªã‚‰ãƒãƒ¼ã‚«ãƒ¼æŒ¿å…¥ãªã—<br/>
                      â†’ 3000msä»¥ä¸Šãªã‚‰ãƒãƒ¼ã‚«ãƒ¼æŒ¿å…¥
                    </span>
                  </span>
                </label>
                <input
                  id="paragraph-break-threshold"
                  type="number"
                  min="500"
                  max="10000"
                  step="500"
                  value={paragraphBreakThreshold}
                  onChange={(e) => {
                    const newValue = parseInt(e.target.value);
                    setParagraphBreakThreshold(newValue);
                    if (isConnected) {
                      sendVadParamsToServer({ paragraph_break_threshold_ms: newValue });
                    }
                  }}
                  disabled={!speechBreakDetection || isRecording}
                  className="block w-full px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                />
                <p className="text-xs text-gray-500 mt-1">
                  500-10000ms (éŸ³å£°åŒºé–“æ¤œå‡ºæ™‚ã®ã¿)
                </p>
              </div>

              {/* Prefix Padding */}
              <div>
                <label htmlFor="vad-padding" className="block text-xs font-medium text-gray-600 mb-1 flex items-center">
                  é–‹å§‹ä½™è£•æ™‚é–“ (ms):
                  <span className="ml-1 relative group">
                    <span className="cursor-help text-blue-500 hover:text-blue-700">?</span>
                    <span className="absolute right-0 bottom-full mb-2 w-72 p-2 bg-gray-800 text-white text-xs rounded shadow-lg opacity-0 group-hover:opacity-100 transition-opacity z-50 pointer-events-none whitespace-normal">
                      ç™ºè©±é–‹å§‹ã¨åˆ¤å®šã•ã‚ŒãŸæ™‚ç‚¹ã‚ˆã‚Šå‰ã®éŸ³å£°ã‚’ã©ã‚Œã ã‘å«ã‚ã‚‹ã‹ã€‚<br/><br/>
                      <strong>çŸ­ã„ (100-200ms):</strong> ç™ºè©±é–‹å§‹ç›´å‰ã®ã¿ã€‚å†’é ­ãŒåˆ‡ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Š<br/><br/>
                      <strong>é•·ã„ (500-1000ms):</strong> ç™ºè©±å‰ã®éŸ³ã‚‚å«ã‚€ã€‚å†’é ­ãŒåˆ‡ã‚Œã«ãã„ãŒãƒã‚¤ã‚ºã‚‚å…¥ã‚Šã‚„ã™ã„
                    </span>
                  </span>
                </label>
                <input
                  id="vad-padding"
                  type="number"
                  min="100"
                  max="1000"
                  step="50"
                  value={vadPrefixPadding}
                  onChange={(e) => {
                    const newValue = parseInt(e.target.value);
                    setVadPrefixPadding(newValue);
                    if (isConnected) {
                      sendVadParamsToServer({ prefix_padding_ms: newValue });
                    }
                  }}
                  disabled={!vadEnabled || isRecording}
                  className="block w-full px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                />
                <p className="text-xs text-gray-500 mt-1">
                  100-1000ms
                </p>
              </div>
            </div>

            <div className="mt-3 flex items-center justify-between">
              <div className="text-xs text-blue-600">
                <strong>æ¨å¥¨:</strong> é«˜ç²¾åº¦ã¯çµ‚äº†æ™‚é–“3000msã€åŒºåˆ‡ã‚Šé‡è¦–ã¯500ms
              </div>
              <button
                onClick={() => {
                  setVadThreshold(0.2);
                  setVadSilenceDuration(3000);
                  setVadPrefixPadding(300);
                  setSkipSilentChunks(false);
                  setAudioBufferSize(4096);
                  setBatchMultiplier(1);
                  // æ¥ç¶šä¸­ãªã‚‰å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é€ä¿¡
                  if (isConnected) {
                    sendVadParamsToServer({
                      threshold: 0.2,
                      silence_duration_ms: 3000,
                      prefix_padding_ms: 300
                    });
                  }
                }}
                disabled={!vadEnabled || isRecording}
                className="px-3 py-1 text-xs bg-green-600 text-white rounded hover:bg-green-700 disabled:bg-gray-400 disabled:cursor-not-allowed transition-colors"
              >
                ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™
              </button>
            </div>

            {/* è¨­å®šæ›´æ–°ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ */}
            {settingsUpdateMessage && (
              <div className="mt-2 p-2 bg-green-100 text-green-700 text-sm rounded-md">
                {settingsUpdateMessage}
              </div>
            )}

            {/* AIå†ç·¨ãƒ¢ãƒ‡ãƒ«è¨­å®š */}
            <div className="mt-6 pt-4 border-t border-gray-200">
              <h4 className="text-sm font-medium text-gray-700 mb-2">AIå†ç·¨ãƒ¢ãƒ‡ãƒ«</h4>
              <select
                value={rewriteModel}
                onChange={(e) => {
                  const newModel = e.target.value;
                  setRewriteModel(newModel);
                  sendRewriteModelToServer(newModel);
                }}
                className="block w-full px-3 py-2 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
              >
                <option value="gpt-4.1-mini">GPT-4.1 Miniï¼ˆé«˜é€Ÿãƒ»ä½ã‚³ã‚¹ãƒˆï¼‰</option>
                <option value="gpt-4.1">GPT-4.1ï¼ˆé«˜ç²¾åº¦ï¼‰</option>
                <option value="gpt-4o-mini">GPT-4o Mini</option>
                <option value="gpt-4o">GPT-4o</option>
              </select>
              <p className="text-xs text-gray-500 mt-1">
                å…±åŒæ ¡æ­£ç”»é¢ã§ã®AIå†ç·¨ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
              </p>
            </div>
          </div>
        </div>

        {/* Connection Status & Session Management - Side by Side */}
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
          {/* Connection Status - æ”¹å–„ã•ã‚ŒãŸæ¥ç¶šçŠ¶æ…‹ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ */}
          <div className="bg-white p-6 rounded-lg shadow-md">
            <h3 className="text-lg font-medium text-gray-900 mb-4">
              OpenAI Realtime API æ¥ç¶šçŠ¶æ…‹
            </h3>
            <div className="space-y-4">
              {/* æ¥ç¶šçŠ¶æ…‹ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ï¼ˆ4çŠ¶æ…‹: æœªæ¥ç¶š/æ¥ç¶šä¸­/æ¥ç¶šæ¸ˆã¿/ã‚¨ãƒ©ãƒ¼ï¼‰ */}
              <div className={`p-4 rounded-lg border-2 ${
                error ? 'bg-red-50 border-red-300' :
                isConnecting ? 'bg-yellow-50 border-yellow-300' :
                isConnected ? 'bg-green-50 border-green-300' :
                'bg-gray-50 border-gray-300'
              }`}>
                <div className="flex items-center space-x-3">
                  <div className={`w-4 h-4 rounded-full ${
                    error ? 'bg-red-500 animate-pulse' :
                    isConnecting ? 'bg-yellow-500 animate-pulse' :
                    isConnected ? 'bg-green-500' :
                    'bg-gray-400'
                  }`}></div>
                  <div>
                    <span className={`text-lg font-medium ${
                      error ? 'text-red-800' :
                      isConnecting ? 'text-yellow-800' :
                      isConnected ? 'text-green-800' :
                      'text-gray-600'
                    }`}>
                      {error ? 'ã‚¨ãƒ©ãƒ¼' :
                       isConnecting ? 'æ¥ç¶šä¸­...' :
                       isConnected ? 'æ¥ç¶šæ¸ˆã¿' :
                       'æœªæ¥ç¶š'}
                    </span>
                    <p className={`text-sm ${
                      error ? 'text-red-600' :
                      isConnecting ? 'text-yellow-600' :
                      isConnected ? 'text-green-600' :
                      'text-gray-500'
                    }`}>
                      {error ? error :
                       isConnecting ? 'OpenAI Realtime APIã«æ¥ç¶šã—ã¦ã„ã¾ã™...' :
                       isConnected ? 'éŸ³å£°å…¥åŠ›ã®æº–å‚™ãŒã§ãã¾ã—ãŸ' :
                       'ã€ŒéŸ³å£°å…¥åŠ›ã§æ–‡å­—èµ·ã“ã—ã€ã¾ãŸã¯ã€ŒéŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã§æ–‡å­—èµ·ã“ã—ã€ã§è‡ªå‹•æ¥ç¶šã—ã¾ã™'}
                    </p>
                  </div>
                </div>
              </div>

              {/* åˆ‡æ–­ãƒœã‚¿ãƒ³ï¼ˆæ¥ç¶šä¸­ã®ã¿è¡¨ç¤ºï¼‰ */}
              {isConnected && (
                <div className="flex items-center justify-center">
                  <button
                    onClick={disconnectWebSocket}
                    className="px-4 py-2 text-sm bg-red-600 hover:bg-red-700 text-white rounded-lg transition-colors"
                  >
                    æ¥ç¶šã‚’åˆ‡æ–­
                  </button>
                </div>
              )}

              {/* è‡ªå‹•åˆ‡æ–­ã®èª¬æ˜ */}
              <p className="text-xs text-gray-500 text-center">
                {isConnected
                  ? `éŸ³å£°å…¥åŠ›åœæ­¢å¾Œ${autoDisconnectDelay}ç§’ã§è‡ªå‹•åˆ‡æ–­ã•ã‚Œã¾ã™`
                  : 'éŸ³å£°å…¥åŠ›é–‹å§‹æ™‚ã«è‡ªå‹•çš„ã«æ¥ç¶šã•ã‚Œã€åœæ­¢å¾Œã«è‡ªå‹•åˆ‡æ–­ã•ã‚Œã¾ã™'}
              </p>

              {/* 60åˆ†ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¶é™ã®æ³¨æ„ */}
              <div className="mt-2 p-2 bg-amber-50 border border-amber-200 rounded text-xs text-amber-700">
                <span className="font-medium">æ³¨æ„:</span> OpenAI Realtime APIã¯1ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ€å¤§60åˆ†ã§ã™ã€‚60åˆ†ã‚’è¶…ãˆã‚‹ã¨è‡ªå‹•åˆ‡æ–­ã•ã‚Œã¾ã™ã€‚å†åº¦ã€ŒéŸ³å£°å…¥åŠ›ã§æ–‡å­—ãŠã“ã—ã€ã‚’æŠ¼ã—ã¦å†æ¥ç¶šã—ã¦ãã ã•ã„ã€‚
              </div>
            </div>
          </div>

          {/* Session Management */}
          <div className="bg-white p-6 rounded-lg shadow-md">
          <h3 className="text-lg font-medium text-gray-900 mb-4">
            å…±åŒæ ¡æ­£ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
          </h3>
          
          {/* Current Session Display */}
          <div className="mb-4">
            <label className="block text-sm font-medium text-gray-700 mb-2">
              ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ID:
            </label>
            {isEditingSessionId ? (
              <div className="flex space-x-2">
                <input
                  type="text"
                  value={sessionIdInput}
                  onChange={(e) => setSessionIdInput(e.target.value)}
                  className="flex-1 px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
                  placeholder="ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’å…¥åŠ›..."
                />
                <button
                  onClick={saveSessionId}
                  className="px-4 py-2 text-sm bg-blue-600 text-white rounded-md hover:bg-blue-700"
                >
                  ä¿å­˜
                </button>
                <button
                  onClick={cancelEditSessionId}
                  className="px-4 py-2 text-sm bg-gray-300 text-gray-700 rounded-md hover:bg-gray-400"
                >
                  ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                </button>
              </div>
            ) : (
              <div className="flex items-center space-x-2">
                <div className="flex-1 px-3 py-2 bg-gray-50 border border-gray-200 rounded-md">
                  <span className="text-gray-700">
                    {currentSessionId || 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“'}
                  </span>
                </div>
                <button
                  onClick={editSessionId}
                  className="px-4 py-2 text-sm bg-blue-600 text-white rounded-md hover:bg-blue-700"
                >
                  å¤‰æ›´
                </button>
              </div>
            )}
          </div>

          {/* Connect to Session */}
          <div className="mb-4">
            <label className="block text-sm font-medium text-gray-700 mb-2">
              ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æ¥ç¶š:
            </label>
            <div className="flex space-x-2">
              <select
                value={existingSessionInput}
                onChange={(e) => setExistingSessionInput(e.target.value)}
                onFocus={fetchSessions}
                className="flex-1 px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500"
              >
                <option value="">ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é¸æŠ...</option>
                <option value="__new__">ï¼‹ æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ</option>
                {activeSessions.map(s => (
                  <option key={s.sessionId} value={s.sessionId}>
                    {s.sessionId} ({s.connectionCount}äººæ¥ç¶šä¸­)
                  </option>
                ))}
              </select>
              <button
                onClick={fetchSessions}
                disabled={isLoadingSessions}
                className="px-3 py-2 text-sm bg-gray-200 text-gray-700 rounded-md hover:bg-gray-300 disabled:opacity-50"
                title="ä¸€è¦§ã‚’æ›´æ–°"
              >
                â†»
              </button>
              <button
                onClick={connectToExistingSession}
                disabled={!existingSessionInput.trim()}
                className="px-4 py-2 text-sm bg-green-600 text-white rounded-md hover:bg-green-700 disabled:bg-gray-400 disabled:cursor-not-allowed"
              >
                æ¥ç¶š
              </button>
            </div>
            {activeSessions.length === 0 && !isLoadingSessions && (
              <p className="text-xs text-gray-500 mt-1">ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“</p>
            )}
          </div>

          {/* Session Status */}
          {currentSessionId && isConnected && (
            <div className="space-y-3">
              <div className="p-3 bg-green-50 border border-green-200 rounded-md">
                <div className="flex items-center space-x-2">
                  <div className="w-3 h-3 bg-green-500 rounded-full"></div>
                  <span className="text-sm font-medium text-green-800">
                    ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¥ç¶šä¸­: {currentSessionId}
                  </span>
                </div>
              </div>
              
              {/* Share Session URL */}
              <div className="p-3 bg-blue-50 border border-blue-200 rounded-md">
                <div className="flex items-center justify-between">
                  <div className="text-sm text-blue-700">
                    ã“ã®URLã‚’å…±æœ‰ã—ã¦ä»–ã®äººã‚’æ‹›å¾…
                  </div>
                  <button
                    onClick={() => {
                      const editorUrl = `${window.location.origin}/editor/${currentSessionId}`;
                      navigator.clipboard.writeText(editorUrl);
                      // Optional: Show feedback (could add a toast notification here)
                      const button = document.activeElement as HTMLButtonElement;
                      const originalText = button.textContent;
                      button.textContent = 'ã‚³ãƒ”ãƒ¼å®Œäº†ï¼';
                      setTimeout(() => {
                        button.textContent = originalText;
                      }, 2000);
                    }}
                    className="px-3 py-1 text-sm bg-blue-600 text-white rounded hover:bg-blue-700 transition-colors"
                  >
                    URLã‚’ã‚³ãƒ”ãƒ¼
                  </button>
                </div>
                <div className="mt-2 text-xs text-blue-600 font-mono">
                  {typeof window !== 'undefined' && `${window.location.origin}/editor/${currentSessionId}`}
                </div>
              </div>
            </div>
          )}
          
          {/* Create/Open Session Button */}
          <div className="flex justify-center space-x-4">
            <button
              onClick={createOrOpenEditingSession}
              className="px-6 py-3 rounded-lg font-medium text-white bg-green-600 hover:bg-green-700 transition-colors"
            >
              {currentSessionId ? 'å…±åŒæ ¡æ­£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹ã' : 'å…±åŒæ ¡æ­£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ'}
            </button>
            
            <button
              onClick={sendTestText}
              disabled={!currentSessionId}
              className="px-6 py-3 rounded-lg font-medium text-white bg-blue-600 hover:bg-blue-700 disabled:bg-gray-400 disabled:cursor-not-allowed transition-colors"
              title={!currentSessionId ? "ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’è¨­å®šã—ã¦ãã ã•ã„" : "ãƒ†ã‚¹ãƒˆæ–‡å­—åˆ—ã‚’å…±åŒæ ¡æ­£ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«é€ä¿¡"}
            >
              ãƒ†ã‚¹ãƒˆæ–‡å­—åˆ—é€ä¿¡
            </button>
          </div>
          </div>
        </div>

        {/* Controls - 2 Column Layout */}
        <div className={`p-6 rounded-lg transition-colors ${
          isRecording
            ? "bg-red-50 border-2 border-red-200"
            : "bg-white"
        }`}>
          <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
            {/* Main Recording Controls */}
            <div className="space-y-4 bg-white p-4 rounded-lg shadow-sm">
              <h4 className="text-md font-medium text-gray-900 text-center">
                éŸ³å£°å…¥åŠ›ã‹ã‚‰ã®æ–‡å­—èµ·ã“ã—
              </h4>

              {/* Audio Input Device Selection */}
              <div className="px-4">
                <label htmlFor="device-select" className="block text-sm font-medium text-gray-700 mb-2">
                  éŸ³å£°å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹:
                </label>
                <select
                  id="device-select"
                  value={selectedDeviceId}
                  onChange={(e) => setSelectedDeviceId(e.target.value)}
                  disabled={isRecording}
                  className="block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500 disabled:bg-gray-100"
                >
                  {audioDevices.length === 0 ? (
                    <option value="">ãƒ‡ãƒã‚¤ã‚¹ã‚’èª­ã¿è¾¼ã¿ä¸­...</option>
                  ) : (
                    audioDevices.map((device) => (
                      <option key={device.deviceId} value={device.deviceId}>
                        {device.label || `ãƒã‚¤ã‚¯ ${device.deviceId.slice(0, 8)}...`}
                      </option>
                    ))
                  )}
                </select>
                <div className="mt-2 flex items-center space-x-2">
                  <button
                    onClick={getAudioDevices}
                    disabled={isRecording}
                    className="px-3 py-1 text-xs bg-gray-200 hover:bg-gray-300 rounded disabled:opacity-50 disabled:cursor-not-allowed"
                  >
                    ãƒ‡ãƒã‚¤ã‚¹æ›´æ–°
                  </button>
                  <p className="text-xs text-gray-500">
                    {audioDevices.length} å€‹ã®ãƒ‡ãƒã‚¤ã‚¹
                  </p>
                </div>
              </div>

              {/* Audio Processing Settings */}
              <div className="px-4 pt-3 border-t border-gray-200">
                <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
                  {/* Audio Buffer Size */}
                  <div>
                    <label htmlFor="buffer-size-select" className="block text-xs font-medium text-gray-700 mb-1">
                      ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º:
                      <span className="ml-1 relative group">
                        <span className="cursor-help text-blue-500 hover:text-blue-700">?</span>
                        <span className="absolute left-0 bottom-full mb-2 w-64 p-2 bg-gray-800 text-white text-xs rounded shadow-lg opacity-0 group-hover:opacity-100 transition-opacity z-50 pointer-events-none whitespace-normal">
                          éŸ³å£°å‡¦ç†1å›ã‚ãŸã‚Šã®ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰ã€‚<br/><br/>
                          <strong>å°ã•ã„å€¤ (256-1024):</strong> ç´°ã‹ã„å‡¦ç†å˜ä½ã€‚CPUè² è·é«˜<br/><br/>
                          <strong>å¤§ãã„å€¤ (8192-16384):</strong> ç²—ã„å‡¦ç†å˜ä½ã€‚CPUè² è·ä½ã€å®‰å®š<br/><br/>
                          â€»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯ãƒãƒƒãƒ•ã‚¡Ã—ãƒãƒƒãƒã§æ±ºã¾ã‚Šã¾ã™
                        </span>
                      </span>
                    </label>
                    <select
                      id="buffer-size-select"
                      value={audioBufferSize}
                      onChange={(e) => setAudioBufferSize(parseInt(e.target.value))}
                      disabled={isRecording}
                      className="w-full px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500 disabled:bg-gray-100"
                    >
                      <option value="256">256 (~5ms)</option>
                      <option value="512">512 (~11ms)</option>
                      <option value="1024">1024 (~21ms)</option>
                      <option value="2048">2048 (~43ms)</option>
                      <option value="4096">4096 (~85ms)</option>
                      <option value="8192">8192 (~171ms)</option>
                      <option value="16384">16384 (~341ms)</option>
                    </select>
                  </div>

                  {/* Batch Multiplier */}
                  <div>
                    <label htmlFor="batch-multiplier-select" className="block text-xs font-medium text-gray-700 mb-1">
                      ãƒãƒƒãƒæ•°:
                      <span className="ml-1 relative group">
                        <span className="cursor-help text-blue-500 hover:text-blue-700">?</span>
                        <span className="absolute right-0 bottom-full mb-2 w-64 p-2 bg-gray-800 text-white text-xs rounded shadow-lg opacity-0 group-hover:opacity-100 transition-opacity z-50 pointer-events-none whitespace-normal">
                          ä½•å›åˆ†ã®ãƒãƒƒãƒ•ã‚¡ã‚’è“„ç©ã—ã¦ã‹ã‚‰é€ä¿¡ã™ã‚‹ã‹ã€‚<br/><br/>
                          <strong>1 (å³æ™‚):</strong> æ¯å›é€ä¿¡ã€‚é€šä¿¡å›æ•°å¤š<br/><br/>
                          <strong>8-16:</strong> ãƒãƒ©ãƒ³ã‚¹å‹ã€‚æ¨å¥¨<br/><br/>
                          <strong>32-64:</strong> ã¾ã¨ã‚ã¦é€ä¿¡ã€‚é€šä¿¡å›æ•°å°‘<br/><br/>
                          â€»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯ãƒãƒƒãƒ•ã‚¡Ã—ãƒãƒƒãƒã§æ±ºã¾ã‚Šã¾ã™
                        </span>
                      </span>
                    </label>
                    <select
                      id="batch-multiplier-select"
                      value={batchMultiplier}
                      onChange={(e) => setBatchMultiplier(parseInt(e.target.value))}
                      disabled={isRecording}
                      className="w-full px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500 disabled:bg-gray-100"
                    >
                      <option value="1">1 (å³æ™‚)</option>
                      <option value="2">2</option>
                      <option value="4">4</option>
                      <option value="8">8</option>
                      <option value="16">16</option>
                      <option value="32">32</option>
                      <option value="64">64</option>
                    </select>
                  </div>
                </div>
                <div className="mt-2">
                  <p className="text-xs text-gray-600">
                    é€ä¿¡é–“éš”: <span className="font-medium">{((audioBufferSize * batchMultiplier) / 24000 * 1000).toFixed(0)}ms</span> ({audioBufferSize} Ã— {batchMultiplier} Ã· 24000 Ã— 1000)
                  </p>
                </div>
              </div>

              {/* Silent Chunk Skip */}
              <div className="px-4">
                <label className="flex items-center">
                  <input
                    type="checkbox"
                    checked={skipSilentChunks}
                    onChange={(e) => setSkipSilentChunks(e.target.checked)}
                    disabled={isRecording}
                    className="mr-2 h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300 rounded"
                  />
                  <span className="text-sm font-medium text-gray-700">
                    ç„¡éŸ³ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
                  </span>
                </label>
                <p className="text-xs text-gray-500 mt-1 ml-6">
                  ç„¡åŠ¹ï¼ˆæ¨å¥¨ï¼‰: é«˜ç²¾åº¦ã€€/ æœ‰åŠ¹: å¸¯åŸŸç¯€ç´„
                </p>
              </div>

              {/* Start/Stop Button */}
              <div className="flex justify-center pt-2">
                <button
                  onClick={isRecording && !isDummyAudioSending ? stopRecording : startRecording}
                  disabled={isConnecting || isDummyAudioSending}
                  className={`px-6 py-3 rounded-lg font-medium text-white transition-colors ${
                    isRecording && !isDummyAudioSending
                      ? "bg-red-600 hover:bg-red-700"
                      : isConnecting
                        ? "bg-yellow-600 cursor-wait"
                        : "bg-blue-600 hover:bg-blue-700 disabled:bg-gray-400 disabled:cursor-not-allowed"
                  }`}
                >
                  {isConnecting
                    ? "æ¥ç¶šä¸­..."
                    : isRecording && !isDummyAudioSending
                      ? "éŸ³å£°å…¥åŠ›ã‚’åœæ­¢"
                      : "éŸ³å£°å…¥åŠ›ã§æ–‡å­—èµ·ã“ã—"}
                </button>
              </div>

              {/* Recording Status Display */}
              {isRecording && !isDummyAudioSending && (
                <div className="flex flex-col items-center justify-center space-y-2 text-blue-600 pt-3">
                  <div className="flex items-center space-x-2">
                    <div className="w-3 h-3 bg-blue-600 rounded-full animate-pulse"></div>
                    <span className="font-medium">Streaming Audio...</span>
                  </div>
                  <div className="flex items-center space-x-2">
                    <span className="text-2xl font-bold tabular-nums">
                      {Math.floor(recordingElapsedTime / 60).toString().padStart(2, '0')}:
                      {Math.floor(recordingElapsedTime % 60).toString().padStart(2, '0')}
                    </span>
                    <span className="text-sm text-gray-500">çµŒé</span>
                  </div>
                </div>
              )}
            </div>

            {/* Dummy Audio Controls */}
            <div className="space-y-4 bg-white p-4 rounded-lg shadow-sm">
              <h4 className="text-md font-medium text-gray-900 text-center">
                éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æ–‡å­—èµ·ã“ã—
              </h4>

              {/* Settings */}
              <div className="space-y-4 px-4">
                {/* Recording Selection */}
                <div className="space-y-2">
                  <label htmlFor="dummy-recording-select" className="block text-sm font-medium text-gray-700">
                    éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿:
                  </label>
                  <div className="flex items-center space-x-2">
                    <select
                      id="dummy-recording-select"
                      value={selectedRecordingId}
                      onChange={(e) => setSelectedRecordingId(e.target.value)}
                      disabled={isDummyAudioSending || isRecording}
                      className="flex-1 px-3 py-2 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500 disabled:bg-gray-100"
                    >
                      {localStorageRecordings.length === 0 ? (
                        <option value="">éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“</option>
                      ) : (
                        localStorageRecordings.map((recording) => {
                          const sizeKB = (recording.data.length * 0.75 / 1024).toFixed(1);
                          // Calculate duration from PCM data size if not available
                          // Base64 length * 0.75 = bytes, bytes / 2 = samples (16-bit), samples / 24000 = seconds
                          const duration = recording.duration || (recording.data.length * 0.75 / 2 / 24000);
                          return (
                            <option key={recording.id} value={recording.id}>
                              {recording.name} ({duration.toFixed(1)}ç§’, {sizeKB}KB)
                            </option>
                          );
                        })
                      )}
                    </select>
                    <button
                      onClick={loadLocalStorageRecordings}
                      disabled={isDummyAudioSending || isRecording}
                      className="px-3 py-1 text-xs bg-gray-200 hover:bg-gray-300 rounded disabled:opacity-50 transition-colors"
                    >
                      æ›´æ–°
                    </button>
                  </div>
                </div>

                {/* Send Interval */}
                <div className="flex items-center space-x-2">
                  <label htmlFor="dummy-send-interval" className="text-sm font-medium text-gray-700">
                    é€ä¿¡é–“éš”:
                  </label>
                  <input
                    id="dummy-send-interval"
                    type="number"
                    min="10"
                    max="500"
                    step="10"
                    value={dummySendInterval}
                    onChange={(e) => setDummySendInterval(Math.max(10, Math.min(500, parseInt(e.target.value) || 50)))}
                    disabled={isDummyAudioSending || isRecording}
                    className="w-20 px-2 py-1 text-sm border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-blue-500 focus:border-blue-500 disabled:bg-gray-100 text-center"
                  />
                  <span className="text-sm text-gray-500">ms</span>
                </div>
              </div>

              {/* Start/Stop Button */}
              <div className="flex justify-center pt-2">
                <button
                  onClick={isDummyAudioSending ? stopDummyAudio : sendDummyAudio}
                  disabled={isConnecting || (isRecording && !isDummyAudioSending) || (!isDummyAudioSending && localStorageRecordings.length === 0)}
                  className={`px-6 py-3 text-sm font-medium rounded-lg transition-colors ${
                    isDummyAudioSending
                      ? "bg-red-600 hover:bg-red-700 text-white"
                      : isConnecting || (isRecording && !isDummyAudioSending) || localStorageRecordings.length === 0
                      ? "bg-gray-400 cursor-not-allowed text-gray-200"
                      : "bg-orange-600 hover:bg-orange-700 text-white"
                  }`}
                >
                  {isDummyAudioSending ? 'éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æ–‡å­—èµ·ã“ã—ã®åœæ­¢' : 'éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã§æ–‡å­—èµ·ã“ã—'}
                </button>
              </div>

              {/* Progress Display */}
              {isDummyAudioSending && (
                <div className="flex flex-col items-center justify-center space-y-2 text-orange-600">
                  <div className="flex items-center space-x-2">
                    <div className="w-4 h-4 border-2 border-orange-600 border-t-transparent rounded-full animate-spin"></div>
                    <span className="text-sm font-medium">éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ä¸­...</span>
                  </div>
                  {dummyAudioProgress && (
                    <div className="flex flex-col items-center space-y-1">
                      <span className="text-lg font-bold">
                        {dummyAudioProgress.currentSeconds.toFixed(1)}ç§’ / {dummyAudioProgress.totalSeconds.toFixed(1)}ç§’
                      </span>
                      <div className="w-64 h-2 bg-gray-200 rounded-full overflow-hidden">
                        <div
                          className="h-full bg-orange-500 transition-all duration-200"
                          style={{ width: `${(dummyAudioProgress.currentSeconds / dummyAudioProgress.totalSeconds) * 100}%` }}
                        />
                      </div>
                    </div>
                  )}
                </div>
              )}
            </div>
          </div>
        </div>

        {/* Error Display */}
        {error && (
          <div className="p-4 bg-red-50 border border-red-200 rounded-lg">
            <div className="flex">
              <div className="ml-3">
                <h3 className="text-sm font-medium text-red-800">Error</h3>
                <div className="mt-2 text-sm text-red-700">
                  {error}
                </div>
              </div>
            </div>
          </div>
        )}

        {/* Transcription Output */}
        <div className="bg-white p-6 rounded-lg shadow-md">
          <div className="flex justify-between items-center mb-4">
            <h2 className="text-lg font-medium text-gray-900">
              æ–‡å­—èµ·ã“ã—çµæœ
            </h2>
            <div className="flex gap-2">
              <button
                onClick={copyText}
                disabled={!text}
                className="px-4 py-2 rounded-lg font-medium text-white bg-blue-600 hover:bg-blue-700 disabled:bg-gray-300 disabled:cursor-not-allowed transition-colors"
              >
                ğŸ“‹ ã‚³ãƒ”ãƒ¼
              </button>
              <button
                onClick={() => setShowClearConfirmDialog(true)}
                disabled={isRecording || isDummyAudioSending || !text}
                className="px-4 py-2 rounded-lg font-medium text-white bg-red-500 hover:bg-red-600 disabled:bg-red-200 disabled:cursor-not-allowed transition-colors"
              >
                ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
              </button>
            </div>
          </div>
          <div className="min-h-[200px] p-4 border border-gray-300 rounded-md bg-gray-50">
            {text ? (
              <p className="text-gray-900 whitespace-pre-wrap leading-relaxed">
                {text}
              </p>
            ) : (
              <p className="text-gray-500 italic">
                Start streaming to see real-time transcription...
              </p>
            )}
            {/* éŸ³å£°æ¤œå‡ºä¸­è¡¨ç¤ºï¼ˆisSpeakingãŒtrueã®é–“ã¯å¸¸ã«è¡¨ç¤ºï¼‰ */}
            {(isRecording || isDummyAudioSending) && isSpeaking && (
              <div className="flex items-center space-x-1 mt-2">
                <div className="w-2 h-2 bg-green-500 rounded-full animate-pulse"></div>
                <span className="text-green-600 text-sm font-medium">æ¤œå‡ºä¸­</span>
              </div>
            )}
            {/* èªè­˜ä¸­è¡¨ç¤ºï¼ˆpendingTextãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤ºï¼‰ */}
            {(isRecording || isDummyAudioSending) && pendingText && (
              <div className="flex items-start space-x-2 mt-1">
                <span className="text-gray-500 text-sm">èªè­˜ä¸­:</span>
                <span className="text-gray-400 italic">{pendingText}</span>
              </div>
            )}
          </div>
          {text && (
            <div className="mt-4 text-sm text-gray-600">
              Characters: {text.length} | Words: {text.split(/\s+/).filter(word => word.length > 0).length}
            </div>
          )}
        </div>

        {/* Instructions */}
        <div className="bg-blue-50 p-6 rounded-lg border border-blue-200">
          <h3 className="text-lg font-medium text-blue-900 mb-3">
            ä½¿ã„æ–¹
          </h3>
          <ol className="list-decimal list-inside space-y-2 text-blue-800">
            <li>éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã¨VADè¨­å®šã‚’èª¿æ•´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰</li>
            <li>ã€ŒéŸ³å£°å…¥åŠ›ã§æ–‡å­—èµ·ã“ã—ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ˆè‡ªå‹•çš„ã«æ¥ç¶šã•ã‚Œã¾ã™ï¼‰</li>
            <li>ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã—ã€è‡ªç„¶ã«è©±ã™</li>
            <li>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ–‡å­—èµ·ã“ã—ãŒè¡¨ç¤ºã•ã‚Œã‚‹</li>
            <li>çµ‚äº†æ™‚ã¯ã€Œæ–‡å­—èµ·ã“ã—ã®åœæ­¢ã€ã‚’ã‚¯ãƒªãƒƒã‚¯</li>
          </ol>
          <div className="mt-4 text-sm text-blue-700">
            <strong>è‡ªå‹•æ¥ç¶šã«ã¤ã„ã¦:</strong> éŸ³å£°å…¥åŠ›é–‹å§‹æ™‚ã«è‡ªå‹•çš„ã«OpenAI APIã«æ¥ç¶šã—ã€åœæ­¢å¾Œ{autoDisconnectDelay}ç§’ã§è‡ªå‹•åˆ‡æ–­ã•ã‚Œã¾ã™ã€‚
          </div>
          <div className="mt-2 text-sm text-blue-600">
            <strong>æ–™é‡‘ç›®å®‰:</strong> ç´„$0.06-0.24/åˆ†ï¼ˆãƒãƒƒãƒå‡¦ç†ã‚ˆã‚Šé«˜ã„ï¼‰
          </div>
        </div>
        </div>
      </main>

      {/* ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªã‚¢ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚° */}
      {showClearConfirmDialog && (
        <div className="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center z-50">
          <div className="bg-white rounded-lg p-6 max-w-md mx-4 shadow-xl">
            <h3 className="text-lg font-bold text-gray-900 mb-4">
              ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã‹ï¼Ÿ
            </h3>
            <p className="text-gray-600 mb-6">
              ã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“ã€‚ã™ã¹ã¦ã®æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚
            </p>
            <div className="flex justify-end space-x-3">
              <button
                onClick={() => setShowClearConfirmDialog(false)}
                className="px-4 py-2 rounded-lg font-medium text-gray-700 bg-gray-200 hover:bg-gray-300 transition-colors"
              >
                ã‚­ãƒ£ãƒ³ã‚»ãƒ«
              </button>
              <button
                onClick={() => {
                  clearText();
                  setShowClearConfirmDialog(false);
                }}
                className="px-4 py-2 rounded-lg font-medium text-white bg-red-500 hover:bg-red-600 transition-colors"
              >
                ã‚¯ãƒªã‚¢ã™ã‚‹
              </button>
            </div>
          </div>
        </div>
      )}
    </div>
  );
}